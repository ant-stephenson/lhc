
\begin{appendices}
\section{Convexity of the AMS metric}
\label{appendix:ams}
The AMS metric is defined as:
\eq{
    \rm{AMS} &= \sqrt{2\left((s+b)\log(1+\frac{s}{b})-s\right)}
}

In the documentation, the implication is that in general, we expect $b \gg s$ giving the approximate AMS as $\rm{AMS} \sim \frac{s}{\sqrt{b}}(1+\mc{O}(\frac{s}{b}))$.

If we directly check the derivatives of the approximate AMS in the large $b$ regime:
\eq{
    \pd_s^2\frac{s}{\sqrt{b}} &= 0\\
    \pd_b^2\frac{s}{\sqrt{b}} &= \frac{3}{4}\frac{s}{b^{5/2}} >0\\
    \pd_{bs}^2 &= -\frac{1}{2b^{3/2}} < 0
}
which implies non-convexity (and non-concavity).

To check more carefully though, calculate the terms of the Hessian of the original and then apply the approximation:
\eq{
    \pd_s\rm{AMS} &= \rm{AMS}^{-1}\log\left(1+\frac{s}{b}\right) \\
    \pd_b\rm{AMS} &= \rm{AMS}^{-1}\left(\log(1+\frac{s}{b}) -\frac{s}{b}\right) \\
    \pd_s^2\rm{AMS} &= \rm{AMS}^{-1}\left(\frac{1}{b+s} - \rm{AMS}^{-2}\log(1+\frac{s}{b})^2\right) \\
    \pd_b^2\rm{AMS} &= \rm{AMS}^{-1}\left(\frac{s^2}{b^2(s+b)} -\rm{AMS}^{-2}(\log(1+\frac{s}{b}) -\frac{s}{b})^2\right) \\
    \pd_{sb}^2\rm{AMS} &= -\rm{AMS}^{-1}\left(\frac{s}{b(s+b)} - \rm{AMS}^{-2}\log(1+\frac{s}{b})(\log(1+\frac{s}{b}) -\frac{s}{b})\right)
}
and then verify whether any of them are ever $<0$.

Using the AMS approximation, and $\log\left(1+\frac{s}{b}\right) \sim \frac{s}{b} - \half\left(\frac{s}{b}\right)^2$ we get the following
\eq{
    \pd_s^2\rm{AMS} &\sim \frac{\sqrt{b}}{s}\left[\frac{1}{(b+s)} - \frac{b}{s^2}(\frac{s^2}{b^2} - \frac{s^3}{b^3})\right] \\
    &= \frac{\sqrt{b}}{s(s+b)} - \frac{1}{s\sqrt{b}} + \frac{1}{b^{3/2}} \\
    &\gtrsim \eps
}
Since in this regime of large $b$ this term is negative, the Hessian cannot be positive definite and hence the metric is non-convex. If the Hessian is \emph{concave} though, we can simply optimize $-\rm{AMS}$, so we need to check the other terms.

\eq{
    \pd_b^2\rm{AMS} &\sim \frac{\sqrt{b}}{s}\left[\frac{s^2}{b^2(s+b)} - \frac{b}{s^2}(\frac{1}{4}\frac{s^4}{b^4})\right] \\
    &= \frac{s}{b^{3/2}(s+b)} - \frac{1}{4}\frac{s}{b^{5/2}} \\
    &\gtrsim \eps
}

\eq{
    \pd_{bs}^2\rm{AMS} &\sim -\frac{\sqrt{b}}{s}\left[\frac{s}{b(s+b)} + \frac{b}{s^2}(\frac{s}{b} -\half\frac{s}{b^2})(-\half\frac{s^2}{b^2})\right] \\
    &= \frac{1}{2b^{3/2}}(1-\half s) - \frac{1}{\sqrt{b}(s+b)} \\
    &< 0
}

where we take $\eps$ to be some suitably small positive number.

If we decide to take some tolerance, $\eps \sim 10^{-6}$ at which to ignore higher order terms in $\frac{s}{b}$ we can check specifically that the claims on the Hessian above hold in that regime.

So, let's assume we can ignore terms of $O(\frac{s}{b})^3$ then for $\eps\sim 10^{-6}$ a ratio of $\frac{s}{b}\sim 10^{-2}$ would suffice; so to test this, let us choose $s=1$ and $b=100$ for simplicity.
With this, we find that $\pd_s^2\rm{AMS} \sim +10^{-6}$, $\pd_b^2\rm{AMS} \sim +10^{-6}$ and $\pd_{bs}^2\rm{AMS} \sim -10^{-4}$ which satisfies the claims above.
Finally, to verify that this is a reasonable assertion, if we check the ratio of $s:b$ in the data (using the weight vector) we find a ratio of approximately $10^{-3}$, which means our assumption was conservative. 


\section{Modelling}

\subsection{Newton's Method for Logistic Regression}
\label{appendix:newton_lr}
From the conditional distribution under the logistic regression model for a single point $\xv \in R^d$, $y \in \{+1,-1\}$ and coefficients $\betav \in R^d$.
\eq{
    p(y\given \xv, \betav) &= \sigma(f(\xv;\betav)\cdot y) \\
    &= (1 + \exp(y\inprod{\xv}{\betav}))^{-1}
}

We want to minimise the negative loglikelihood:
\eq{
    l(\xv, y, \betav) &= \log(1 + \e^{y\inprod{\xv}{\xv}}) \\
    \pd_{\beta_a} l &= \frac{yx_a\e^{y\inprod{\xv}{\betav}}}{1 + \e^{y\inprod{\xv}{\betav}}} \\
    \pd^2_{\beta_a \beta_b}l &= \frac{y^2x_ax_b\e^{y\inprod{\xv}{\betav}}}{(1+\e^{y\inprod{\xv}{\betav}})^2}\left( 1 + \e^{y\inprod{\xv}{\betav}} - \e^{y\inprod{\xv}{\betav}}\right) \\
    &= \frac{x_ax_b\e^{y\inprod{\xv}{\betav}}}{(1+\e^{y\inprod{\xv}{\betav}})^2}\\
    &= x_ax_bw(x)
}
where in the last line we define a weight function $w(x) = \frac{\e^{-x}}{(1+\e^{-x})^2}$ (and the elementwise vector version, $\wv(\xv) = \frac{\e^{-\xv}}{(1+\e^{-\xv})^2}$).

To implement Newton's method we want to try and simplify the expressions. First define the logistic and logit functions as
\eq{
    \gamma(x) &= (1+\e^{-x})^{-1} \\
    \gamma^{-1}(p) &= \log\left(\frac{p}{1-p}\right)
}
and then write out the derivatives in terms of the logistic function:
\eq{
    \pd_{\beta_a} l(y=+1) &= x_a\gamma(\inprod{\xv}{\betav}) \\
    \pd_{\beta_a} l(y=-1) &= -x_a(1-\gamma(\inprod{\xv}{\betav})
}
If we then map $y \in \{1,-1\}$ to $y' \in \{0,1\}$ we can condense these into a single expression
\eq{
    \pd_{\beta_a} l(y) &= -x_a(y - \gamma(\inprod{\xv}{\betav}))
}
and rewrite the loss as $l(y') = \log(1+\e^{(-1)^{y'}\inprod{\xv}{\betav}})$.

Now for our entire dataset $D$, assumed to be IID, we have a total loss, gradient and Hessian
\eq{
    L &= \sum_{i \in D}l_i = \sum_{i \in D}\log(1+\e^{y_i\inprod{\xv_i}{\betav}})\\
    \pd_{\betav}L &= -\sum_{i \in D}\xv_i(y_i - \gamma(\inprod{\xv_i}{\betav})) \\
    &= -\Xv^T(\yv-\gamma(\Xv\betav)) \\
    \bm{H} &= \sum_{i \in D}\xv_i\xv_i^Tw(\xv_i)\\
    &= \Xv^T\bm{W}\Xv
}
with $\Xv = \bp \xv_1^T \\ \vdots \\ \xv_n^T \ep \in R^{n\times d}$, $\bm{W} = \diag(\wv(\xv)) \in R^{n\times n}$.

\subsubsection{Regularisation}
We can add constraints to our minimisation problem, to help constrain the coefficients and effectively implement $L_2$ regularisation by formulating the following convex optimisation problem.
\eq{
    \min. &\quad L \\
    \st &\quad \norm{\betav}^2\leq c
}
From this we can construct the Lagrangian $\mc{L} = L + \lambda(\norm{\betav}^2 - c)$ and hence augment the gradient and Hessian we calculated above as
\eq{
    \pd_{\betav}\mc{L} &= \pd_{\betav}L + 2\lambda\betav \\
    \bm{\tilde{H}} &= \bm{H} + 2\lambda\Iv
}

Since we have a free parameter $c$ that determines the optimal value $\lambda^\star(c)$ of our dual variable, we can equivalently ignore $c$ and treat $\lambda$ as a parameter to be tuned.

\subsubsection{Newton's method}

\begin{algorithm}[H]
    1. Initialise parameters; $\betav_0$, $\lambda_0$, $\eps$, $L_0 = L(\betav_0)$, $m$\\
    2. \For{i=1:m}{
        Compute $\pd_{\betav}L$ and $\bm{H}$\\
        Calculate Newton step $\Delta\betav = -\bm{H}^{-1}\pd_{\betav}L$\\
        Calculate stepsize $\alpha$ with backtracking linesearch (\ref{algo:line}) \\
        Update $\betav \leftarrow \betav + \alpha\Delta\betav$ \\
        Compute new loss $L$ \\
        Test stopping criterion: $|L-L_0| < \eps$
    } 
    3. Return $\hat{\betav}$.\\
    \caption{Newton's method}
\end{algorithm}
\label{algo:newton}

\begin{algorithm}[H]
    1. Initialise parameters; $\gamma, \tau$ \\
    2. Set largest stepsize $s_{max}\leftarrow 1$ \\
    3. Set $s \leftarrow s_{max}$ \\
    4. \While{$f(x+\Delta x) > f(x) + \gamma s \nabla f^T\Delta x$} {
        Update $s\leftarrow \tau s$ \\
    }
    5. Return $s$
    \caption{Backtracking linesearch}
\end{algorithm}
\label{algo:line}

\subsection{SVM}
\subsubsection{Soft-margin SVM}
For a dataset $D=\{(\textbf{x}_i, y_i)\}_{i=1}^n$, $\textbf{x} \in \mathbb{R}^d$, $y_i \in \{-1, +1\}$, support vector machines (SVM) aim to find the optimal hyperplane which separates samples of each class $y$. The optimal hyperplane $f(\textbf{x}, \textbf{w}) = \textbf{w}^T \textbf{x} + w_0 = 0$ classifies all samples correctly and has the largest distance to the nearest sample. The parameters $\textbf{w}$ and $w_0$ are found by solving the following optimisation problem:
\begin{align}
\text{minimise: } & \frac{1}{2} || \textbf{w} ||^2 \notag \\
\text{subject to: } &\forall_i, \quad y_i f(\textbf{x}_i, \textbf{w}) \geq 0 \notag
\end{align}
When classes are not linearly separable, soft-margin SVM allows some samples to be misclassified by the boundary by including slack variables $\epsilon_i$. The slack variables are penalised, and parameter $C$ controls the strength of this penalty compared to the margin. The optimisation problem becomes:
\begin{align}
\text{minimise: } & \frac{1}{2} || \textbf{w} ||^2 + C \sum_i \epsilon_i \notag \\
\text{subject to: } &\forall_i, \quad y_i f(\textbf{x}_i, \textbf{w}) + \epsilon_i \geq 1, \quad\epsilon_i \geq 1 \notag
\end{align}
The problem can be re-written as a Lagrangian:
\begin{align}
L(\boldsymbol{\lambda}, \boldsymbol{\eta}) 
	&= \frac{1}{2} ||\textbf{w}||^2 
	+ C \sum_i \epsilon_i 
	- \sum_i \lambda_i (y_i (\textbf{w}^T \textbf{x} + w_0) + \epsilon_i - 1) 
	- \sum_i \eta_i \epsilon_i 
	\notag
\end{align}
Setting the differentials to 0 gives the following conditions:
\begin{align}
\textbf{w} = \sum_i \lambda_i y_i \textbf{x}_i^T,
	\qquad
\sum_i \lambda_i y_i = 0,
	\qquad
\lambda_i + \eta_i = C \notag
\end{align}
and substituting these back into the Lagrangian eliminates $\textbf{w}, w_0, \epsilon_i$ and $\eta_i$ to give:
$$L(\boldsymbol{\lambda}) = - \dfrac{||\sum_i \lambda_i y_i \textbf{x}_i^T||^2}{2}
	+ \sum_i \lambda_i$$
This leaves a quadratic optimisation problem to find $\boldsymbol{\lambda}$ subject to the KKT conditions:
$$0 \leq \lambda_i \leq C, \qquad \sum_i \lambda_i y_i = 0$$
To use a quadratic optimisation package, it is useful to rewrite the problem in the form $\boldsymbol{\lambda}^T \textbf{D} \boldsymbol{\lambda}$. Letting $\textbf{X}$ be a matrix with rows $\textbf{x}_i^T$, $\textbf{y}$ be an $n \times 1$ vector, and $\circ$ be a Hadamard product:
\begin{align}\label{SVM Lagrangian}
L(\boldsymbol{\lambda}) =- \dfrac{\boldsymbol{\lambda} ((\textbf{yy}^T) \circ (\textbf{XX}^T)) \boldsymbol{\lambda}}{2} + \langle \textbf{1}, \boldsymbol{\lambda} \rangle
\end{align}
Once $\boldsymbol{\lambda}$ is found, $\textbf{w}$ can be retrieved using $\textbf{w} = \sum_i \lambda_i y_i \textbf{x}_i^T$, and $w_0$ can be retrieved using the support vectors. Let $\mathcal{S}$ be the set of support vectors: samples where $\lambda_i \neq 0$ and $y_i (\textbf{w}^T \textbf{x} + w_0) = 1$. Any support vector can be used to find $w_0$, but it is more numerically stable to find the average across the set:
$$w_0 = \frac{1}{|\mathcal{S}|} \sum_{m \in \mathcal{S}} y_m - \textbf{w}^T \textbf{x}_m$$

Given a new sample $\textbf{x}_n$, it's class is predicted as:
$$\hat{y}_n = \text{sign} (\textbf{w}^T \textbf{x}_n + w_0)$$

\subsubsection{Kernel SVM}
In equation \ref{SVM Lagrangian} the input data appears only in the term $\textbf{XX}^T$, which is equivalent to a linear kernel matrix $\textbf{K}$, where $K_{ij} = k(\textbf{x}_i, \textbf{x}_j) = \textbf{x}_i^T \textbf{x}_j$. Other kernel functions can also be used such as a b-degree polynomial kernel $(\textbf{x}_i^T \textbf{x}_j +1)^b$.

Given a new sample $\textbf{x}_n$, it's class is predicted as:
$$\hat{y}_n = \text{sign} \Big(\sum_{m \in \mathcal{S}} \lambda_m y_m k(\textbf{x}_n, \textbf{x}_m) + w_0 \Big)$$
where $w_0$ is calculated as:
$$w_0 = \frac{1}{|\mathcal{S}|} \sum_{s \in \mathcal{S}} \Big( y_s - \sum_{m \in \mathcal{S}} \lambda_m y_m k(\textbf{x}_s, \textbf{x}_m) \Big)$$

\end{appendices}