
\begin{appendices}
\section{AMS}
Is the AMS metric convex?
\eq{
    \rm{AMS} &= \sqrt{2(s+b)\log(1+\frac{s}{b})-s}
}

In the documentation, the implication is that in general, we expect $b \geqq s$ giving the approximate AMS as $\rm{AMS} \sim \frac{s}{\sqrt{b}}(1+\mc{O}(\frac{s}{b}))$.

If we directly check the derivatives of the approximate AMS in the large $b$ regime:
\eq{
    \pd_s^2\frac{s}{\sqrt{b}} &= 0\\
    \pd_b^2\frac{s}{\sqrt{b}} &= \frac{3}{4}\frac{s}{b^{5/2}} >0\\
    \pd_{bs}^2 &= -\frac{1}{2b^{3/2}} < 0
}
which implies non-convexity (and non-concavity).

To check more carefully though, calculate the terms of the Hessian of the original and then apply the approximation:
\eq{
    \pd_s\rm{AMS} &= \log(1+\frac{s}{b})\rm{AMS}^{-1} \\
    \pd_b\rm{AMS} &= (\log(1+\frac{s}{b}) + sb)\rm{AMS}^{-1} \\
    \pd_s^2\rm{AMS} &= \frac{1}{b+s}\rm{AMS}^{-1} - \log(1+\frac{s}{b})^2\rm{AMS}^{-3} \\
    \pd_b^2\rm{AMS} &= (\frac{sb}{s+b}+s)\rm{AMS}^{-1} - (\log(1+\frac{s}{b}) + sb)^2\rm{AMS}^{-3} \\
    \pd_{sb}^2 &= (\frac{1}{s+b}+b)\rm{AMS}^{-1} - \log(1+\frac{s}{b})(\log(1+\frac{s}{b}) + sb)\rm{AMS}^{-3}
}
and then verify whether any of them are ever $<0$.

Using the AMS approximation, and $\log\left(1+\frac{s}{b}\right) \sim \frac{s}{b} - \half\left(\frac{s}{b}\right)^2$ we get the following
\eq{
    \pd_s^2\rm{AMS} &\sim \frac{1}{b+s}\frac{\sqrt{b}}{s} - \frac{b^{3/2}}{s^3}\frac{s^2}{b^2} \\
    &\sim \frac{1}{sb^{3/2}} - \frac{1}{sb^\half} \\
    < 0
}
Since in this regime of large $b$ this term is negative, the Hessian cannot be positive definite and hence the metric is non-convex. If the Hessian is \emph{concave} though, we can simply optimize $-\rm{AMS}$, so we need to check the other terms.

\eq{
    \pd_b^2\rm{AMS} &\sim \frac{sb}{s+b}\frac{\sqrt{b}}{s} + \sqrt{b} - \frac{b^{3/2}}{s^3}\left(\frac{s}{b} - \half\bigg(\frac{s}{b}\bigg)^2 + sb\right)^2 \\
    &= \frac{b^{3/2}}{s+b} + \sqrt{b} - \frac{1}{s\sqrt{b}} - \frac{b^{7/2}}{s} - 2\frac{b^{3/2}}{s} + \sqrt{b} < 0
}

\eq{
    \pd_{bs}^2\rm{AMS} &\sim \frac{\sqrt{b}}{s(s+b)}+\frac{b^{3/2}}{s} - \frac{b^{3/2}}{s^3}\left(\frac{s}{b}-\half\bigg(\frac{s}{b}\bigg)^2\right)\left(\frac{s}{b}-\half\bigg(\frac{s}{b}\bigg)^2 + sb\right) \\
    &\sim \frac{1}{s\sqrt{b}}\left(1-\frac{s}{b}\right) + \frac{b^{3/2}}{s} - \frac{1}{s\sqrt{b}} + \half\sqrt{b} - \frac{b^{3/2}}{s} \\
    &\sim -\frac{1}{b^{3/2}} + \half\sqrt{b} > 0
}

\section{Modelling}

\subsection{Newton's Method for Logistic Regression}

From the conditional distribution under the logistic regression model for a single point $\xv \in R^d$, $y \in \{+1,-1\}$ and coefficients $\betav \in R^d$.
\eq{
    p(y\given \xv, \betav) &= \sigma(f(\xv;\betav)\cdot y) \\
    &= (1 + \exp(y\inprod{\xv}{\betav}))^{-1}
}

We want to minimise the negative loglikelihood:
\eq{
    l(\xv, y, \betav) &= \log(1 + \e^{y\inprod{\xv}{\xv}}) \\
    \pd_{\beta_a} l &= \frac{yx_a\e^{y\inprod{\xv}{\betav}}}{1 + \e^{y\inprod{\xv}{\betav}}} \\
    \pd^2_{\beta_a \beta_b}l &= \frac{y^2x_ax_b\e^{y\inprod{\xv}{\betav}}}{(1+\e^{y\inprod{\xv}{\betav}})^2}\left( 1 + \e^{y\inprod{\xv}{\betav}} - \e^{y\inprod{\xv}{\betav}}\right) \\
    &= \frac{x_ax_b\e^{y\inprod{\xv}{\betav}}}{(1+\e^{y\inprod{\xv}{\betav}})^2}\\
    &= x_ax_bw(x)
}
where in the last line we define a weight function $w(x) = \frac{\e^x}{(1+\e^x)^2}$.

To implement Newton's method we want to try and simplify the expressions. First define the logistic and logit functions as
\eq{
    \gamma(x) &= (1+\e^{-x})^{-1} \\
    \gamma^{-1}(p) &= \log\left(\frac{p}{1-p}\right)
}
and then write out the derivatives in terms of the logistic function:
\eq{
    \pd_{\beta_a} l(y=+1) &= x_a\gamma(\inprod{\xv}{\betav}) \\
    \pd_{\beta_a} l(y=-1) &= -x_a(1-\gamma(\inprod{\xv}{\betav})
}
If we then map $y \in \{1,-1\}$ to $y' \in \{0,1\}$ we can condense these into a single expression
\eq{
    \pd_{\beta_a} l(y) &= -x_a(y - \gamma(\inprod{\xv}{\betav}))
}
and rewrite the loss as $l(y') = \log(1+\e^{(-1)^{y'}\inprod{\xv}{\betav}})$.

Now for our entire dataset $D$, assumed to be IID, we have a total loss, gradient and Hessian
\eq{
    L &= \sum_{i \in D}l_i = \sum_{i \in D}\log(1+\e^{y_i\inprod{\xv_i}{\betav}})\\
    \pd_{\betav}L &= -\sum_{i \in D}\xv_i(y_i - \gamma(\inprod{\xv_i}{\betav})) \\
    &= -\Xv^T(\yv-\gamma(\Xv\betav)) \\
    \bm{H} &= \sum_{i \in D}\xv_i\xv_i^Tw(\xv_i)\\
    &= \Xv^T\bm{W}\Xv
}
with $\Xv = \bp \xv_1^T \\ \vdots \\ \xv_n^T \ep \in R^{n\times d}$, $\bm{W} = \diag(w(\xv)) \in R^{n\times n}$.

\subsubsection{Regularisation}
We can add constraints to our minimisation problem, to help constrain the coefficients and effectively implement $L_2$ regularisation by formulating the following convex optimisation problem.
\eq{
    \min. &\quad L \\
    \st &\quad \norm{\betav}^2\leq c
}
From this we can construct the Lagrangian $\mc{L} = L + \lambda(\norm{\betav}^2 - c)$ and hence augment the gradient and Hessian we calculated above as
\eq{
    \pd_{\betav}\mc{L} &= \pd_{\betav}L + 2\lambda\betav \\
    \bm{\tilde{H}} &= \bm{H} + 2\lambda\Iv
}

Since we have a free parameter $c$ that determines the optimal value $\lambda^\star(c)$ of our dual variable, we can equivalently ignore $c$ and treat $\lambda$ as a parameter to be tuned.

\subsubsection{Newton's method}

\begin{algorithm}[H]
    1. Initialise parameters; $\betav_0$, $\lambda_0$, $\eps$, $L_0 = L(\betav_0)$, $m$\\
    2. \For{i=1:m}{
        Compute $\pd_{\betav}L$ and $\bm{H}$\\
        Calculate Newton step $\Delta\betav = -\bm{H}^{-1}\pd_{\betav}L$\\
        Compute new loss $L$ \\
        Test stopping criterion: $|L-L_0| < \eps$
    } 
    3. Return $\hat{\betav}$.
    \caption{Newton's method}
\end{algorithm}

\end{appendices}