
\begin{appendices}
\section{AMS}
\label{appendix:ams}
Is the AMS metric convex?
\eq{
    \rm{AMS} &= \sqrt{2\left((s+b)\log(1+\frac{s}{b})-s\right)}
}

In the documentation, the implication is that in general, we expect $b \gg s$ giving the approximate AMS as $\rm{AMS} \sim \frac{s}{\sqrt{b}}(1+\mc{O}(\frac{s}{b}))$.

If we directly check the derivatives of the approximate AMS in the large $b$ regime:
\eq{
    \pd_s^2\frac{s}{\sqrt{b}} &= 0\\
    \pd_b^2\frac{s}{\sqrt{b}} &= \frac{3}{4}\frac{s}{b^{5/2}} >0\\
    \pd_{bs}^2 &= -\frac{1}{2b^{3/2}} < 0
}
which implies non-convexity (and non-concavity).

To check more carefully though, calculate the terms of the Hessian of the original and then apply the approximation:
\eq{
    \pd_s\rm{AMS} &= \rm{AMS}^{-1}\log\left(1+\frac{s}{b}\right) \\
    \pd_b\rm{AMS} &= \rm{AMS}^{-1}\left(\log(1+\frac{s}{b}) -\frac{s}{b}\right) \\
    \pd_s^2\rm{AMS} &= \rm{AMS}^{-1}\left(\frac{1}{b+s} - \rm{AMS}^{-2}\log(1+\frac{s}{b})^2\right) \\
    \pd_b^2\rm{AMS} &= \rm{AMS}^{-1}\left(\frac{s^2}{b^2(s+b)} -\rm{AMS}^{-2}(\log(1+\frac{s}{b}) -\frac{s}{b})^2\right) \\
    \pd_{sb}^2\rm{AMS} &= -\rm{AMS}^{-1}\left(\frac{s}{b(s+b)} - \rm{AMS}^{-2}\log(1+\frac{s}{b})(\log(1+\frac{s}{b}) -\frac{s}{b})\right)
}
and then verify whether any of them are ever $<0$.

Using the AMS approximation, and $\log\left(1+\frac{s}{b}\right) \sim \frac{s}{b} - \half\left(\frac{s}{b}\right)^2$ we get the following
\eq{
    \pd_s^2\rm{AMS} &\sim \frac{\sqrt{b}}{s}\left[\frac{1}{(b+s)} - \frac{b}{s^2}(\frac{s^2}{b^2} - \frac{s^3}{b^3})\right] \\
    &= \frac{\sqrt{b}}{s(s+b)} - \frac{1}{s\sqrt{b}} + \frac{1}{b^{3/2}} \\
    &\gtrsim \eps
}
Since in this regime of large $b$ this term is negative, the Hessian cannot be positive definite and hence the metric is non-convex. If the Hessian is \emph{concave} though, we can simply optimize $-\rm{AMS}$, so we need to check the other terms.

\eq{
    \pd_b^2\rm{AMS} &\sim \frac{\sqrt{b}}{s}\left[\frac{s^2}{b^2(s+b)} - \frac{b}{s^2}(\frac{1}{4}\frac{s^4}{b^4})\right] \\
    &= \frac{s}{b^{3/2}(s+b)} - \frac{1}{4}\frac{s}{b^{5/2}} \\
    &\gtrsim \eps
}

\eq{
    \pd_{bs}^2\rm{AMS} &\sim -\frac{\sqrt{b}}{s}\left[\frac{s}{b(s+b)} + \frac{b}{s^2}(\frac{s}{b} -\half\frac{s}{b^2})(-\half\frac{s^2}{b^2})\right] \\
    &= \frac{1}{2b^{3/2}}(1-\half s) - \frac{1}{\sqrt{b}(s+b)} \\
    &< 0
}

where we take $\eps$ to be some suitably small positive number.

If we decide to take some tolerance, $\eps \sim 10^{-6}$ at which to ignore higher order terms in $\frac{s}{b}$ we can check specifically that the claims on the Hessian above hold in that regime.

So, let's assume we can ignore terms of $O(\frac{s}{b})^3$ then for $\eps\sim 10^{-6}$ a ratio of $\frac{s}{b}\sim 10^{-2}$ would suffice; so to test this, let us choose $s=1$ and $b=100$ for simplicity.
With this, we find that $\pd_s^2\rm{AMS} \sim +10^{-6}$, $\pd_b^2\rm{AMS} \sim +10^{-6}$ and $\pd_{bs}^2\rm{AMS} \sim -10^{-4}$ which satisfies the claims above.
Finally, to verify that this is a reasonable assertion, if we check the ratio of $s:b$ in the data (using the weight vector) we find a ratio of approximately $10^{-3}$, which means our assumption was conservative. 


\section{Modelling}

\subsection{Newton's Method for Logistic Regression}
\label{appendix:newton_lr}
From the conditional distribution under the logistic regression model for a single point $\xv \in R^d$, $y \in \{+1,-1\}$ and coefficients $\betav \in R^d$.
\eq{
    p(y\given \xv, \betav) &= \sigma(f(\xv;\betav)\cdot y) \\
    &= (1 + \exp(y\inprod{\xv}{\betav}))^{-1}
}

We want to minimise the negative loglikelihood:
\eq{
    l(\xv, y, \betav) &= \log(1 + \e^{y\inprod{\xv}{\xv}}) \\
    \pd_{\beta_a} l &= \frac{yx_a\e^{y\inprod{\xv}{\betav}}}{1 + \e^{y\inprod{\xv}{\betav}}} \\
    \pd^2_{\beta_a \beta_b}l &= \frac{y^2x_ax_b\e^{y\inprod{\xv}{\betav}}}{(1+\e^{y\inprod{\xv}{\betav}})^2}\left( 1 + \e^{y\inprod{\xv}{\betav}} - \e^{y\inprod{\xv}{\betav}}\right) \\
    &= \frac{x_ax_b\e^{y\inprod{\xv}{\betav}}}{(1+\e^{y\inprod{\xv}{\betav}})^2}\\
    &= x_ax_bw(x)
}
where in the last line we define a weight function $w(x) = \frac{\e^{-x}}{(1+\e^{-x})^2}$ (and the elementwise vector version, $\wv(\xv) = \frac{\e^{-\xv}}{(1+\e^{-\xv})^2}$).

To implement Newton's method we want to try and simplify the expressions. First define the logistic and logit functions as
\eq{
    \gamma(x) &= (1+\e^{-x})^{-1} \\
    \gamma^{-1}(p) &= \log\left(\frac{p}{1-p}\right)
}
and then write out the derivatives in terms of the logistic function:
\eq{
    \pd_{\beta_a} l(y=+1) &= x_a\gamma(\inprod{\xv}{\betav}) \\
    \pd_{\beta_a} l(y=-1) &= -x_a(1-\gamma(\inprod{\xv}{\betav})
}
If we then map $y \in \{1,-1\}$ to $y' \in \{0,1\}$ we can condense these into a single expression
\eq{
    \pd_{\beta_a} l(y) &= -x_a(y - \gamma(\inprod{\xv}{\betav}))
}
and rewrite the loss as $l(y') = \log(1+\e^{(-1)^{y'}\inprod{\xv}{\betav}})$.

Now for our entire dataset $D$, assumed to be IID, we have a total loss, gradient and Hessian
\eq{
    L &= \sum_{i \in D}l_i = \sum_{i \in D}\log(1+\e^{y_i\inprod{\xv_i}{\betav}})\\
    \pd_{\betav}L &= -\sum_{i \in D}\xv_i(y_i - \gamma(\inprod{\xv_i}{\betav})) \\
    &= -\Xv^T(\yv-\gamma(\Xv\betav)) \\
    \bm{H} &= \sum_{i \in D}\xv_i\xv_i^Tw(\xv_i)\\
    &= \Xv^T\bm{W}\Xv
}
with $\Xv = \bp \xv_1^T \\ \vdots \\ \xv_n^T \ep \in R^{n\times d}$, $\bm{W} = \diag(\wv(\xv)) \in R^{n\times n}$.

\subsubsection{Regularisation}
We can add constraints to our minimisation problem, to help constrain the coefficients and effectively implement $L_2$ regularisation by formulating the following convex optimisation problem.
\eq{
    \min. &\quad L \\
    \st &\quad \norm{\betav}^2\leq c
}
From this we can construct the Lagrangian $\mc{L} = L + \lambda(\norm{\betav}^2 - c)$ and hence augment the gradient and Hessian we calculated above as
\eq{
    \pd_{\betav}\mc{L} &= \pd_{\betav}L + 2\lambda\betav \\
    \bm{\tilde{H}} &= \bm{H} + 2\lambda\Iv
}

Since we have a free parameter $c$ that determines the optimal value $\lambda^\star(c)$ of our dual variable, we can equivalently ignore $c$ and treat $\lambda$ as a parameter to be tuned.

\subsubsection{Newton's method}

\begin{algorithm}[H]
    1. Initialise parameters; $\betav_0$, $\lambda_0$, $\eps$, $L_0 = L(\betav_0)$, $m$\\
    2. \For{i=1:m}{
        Compute $\pd_{\betav}L$ and $\bm{H}$\\
        Calculate Newton step $\Delta\betav = -\bm{H}^{-1}\pd_{\betav}L$\\
        Calculate stepsize $\alpha$ with backtracking linesearch (\ref{algo:line}) \\
        Update $\betav \leftarrow \betav + \alpha\Delta\betav$ \\
        Compute new loss $L$ \\
        Test stopping criterion: $|L-L_0| < \eps$
    } 
    3. Return $\hat{\betav}$.\\
    \caption{Newton's method}
\end{algorithm}
\label{algo:newton}

\begin{algorithm}[H]
    1. Initialise parameters; $\gamma, \tau$ \\
    2. Set largest stepsize $s_{max}\leftarrow 1$ \\
    3. Set $s \leftarrow s_{max}$ \\
    4. \While{$f(x+\Delta x) > f(x) + \gamma s \nabla f^T\Delta x$} {
        Update $s\leftarrow \tau s$ \\
    }
    5. Return $s$
    \caption{Backtracking linesearch}
\end{algorithm}
\label{algo:line}

\end{appendices}