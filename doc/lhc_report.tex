\input{compass_preamble}

\usepackage[toc,page]{appendix}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{pifont}
% \setlist{nolistsep,leftmargin=*}

% \usepackage{graphicx}
% \graphicspath{{/figs}}
% \DeclareGraphicsExtensions{.png, .pdf, .jpeg, .jpg, .eps}

\pagestyle{fancy}
\rhead{}
\lhead{}

\author{Mansell, Georgie \and Stephenson, Ant}
\date{\today}
\title{Identifying fermionic decay signals of the Higgs boson with classification algorithms}

\begin{document}
\maketitle
\begin{abstract}
    ...
\end{abstract}

\section{Introduction}
After the theoretical discovery of the Higgs mechanism and the associated particle, the Higgs boson, experimental confirmation of the prediction became a much sought-after prize in High Energy Physics. Although the theory was more or less accepted by the theoretical community since its inception in the 1960s, experimental evidence was required for verification. 

Once CERN's Large Hadron Collider (LHC) emerged on the scene, with the necessary power to probe higher energy regimes of collisions, vast amounts of data started to be collected. In order to verify the theoretical predictions of particle physics, this data needed to be anaylsed carefully to try and distinguish signal of as-yet unknown particle decays vs effectively ``proven'' existing decays. The accepted threshold for a discovery in particle physics is the ``gold-standard'' 5-sigma rule. As a result, any announcement of a new experimental discovery (such as the Higgs boson) was required to meet this threshold. 

For the Higgs boson in particular, various decay mechanisms were proposed that could be used to demonstrate its existence by comparing the rate of the by-products of those decays vs background mechanisms (that do not involve the Higgs). In 2013, when the Higgs was experimentally discovered by CERN, the evidence was provided by \emph{bosonic} decays from the Higgs to the following pairs, $\gamma\gamma$, $WW$ and $ZZ$. 

The goal here is to instead examine the coupling of the Higgs to fermions, to verify that their mass can likewise be explained by the Higgs mechanism. Specifically, the aim is to analyse the decay $H\rightarrow\tau\tau$; i.e. the decay of the Higgs to a pair of tau-flavour leptons. (The other candidates, electrons and muons fall outside the energy range of the LHC due to their lighter mass).

In this document we look at the problem of improving the statistical significance of the experimental results collected by implementing classification models to identify signal events. We organise the report by providing a brief overview of the dataset and some of its key properties in \ref{data}. After this we introduce our proposed methods for studying the problem and generating classification predictions in \ref{method}. We go onto analyse the results we obtained from running experiments on the data with our models in order to try and pick the best performing model from our subset of trials, being careful to fairly assess this from two key metrics (\ref{results}). Finally we summarise our key findings and overall score on a hold-out test set of data in \ref{conclusion}.

\section{Data}
\label{data}
\subsection{Structure}
We have access to a dataset comprising roughly of primitive covariates, derived covariates, target labels and auxiliary data (weights, subset labels). 

\subsection{Class Imbalance}
Since the events we are looking for in the data are rare, if we were to na\"{\i}vely include rows of signal and background events at the actually observed signal:noise ratio (of approximately 1:1000) we would have a highly imbalanced dataset. As a result, we need to adopt a strategy to address this by either modifying our dataset such that standard classification algorithms can still be of use, or modifying the algorithms to take this into account. Fortunately, the dataset includes a column of weights which account for the imbalance. In particular, the ratio of signal:background rows in the training data is approximately 1:1.9 with associated weights of $O(10^{-3})$ and $O(1)$ for the signal and background rows respectively.

\subsection{Missing Data}
Early examinations of the dataset revealed the presence of a significant quantity of missing data. Elements of the data matrix recorded as missing were labelled by a value of -999. Before embarking on any model building or even feature engineering we analysed the missing values to try and ascertain whether there was any pattern to their locations. From the definitions of covariates in the dataset it was possible to infer potential causes of missing data. In particular, the estimated mass of the Higgs ($DER\_mass\_MMC$) ``(may be undefined if the topology of the event is too far from the expected topology)", indicates that this feature may be expected to be labelled as missing. Similarly, a set of the features mention a dependence on the number of jets measured in the interaction:

\begin{description}
    \item[$DER\_deltaeta\_jet\_jet$] undefined if $PRI\_jet\_num \leq 1$
    \item[$DER\_mass\_jet\_jet$] undefined if $PRI\_jet\_num \leq 1$
    \item[$DER\_prodeta\_jet\_jet$] undefined if $PRI\_jet\_num \leq 1$
    \item[$PRI\_jet\_subleading\_pt$] undefined if $PRI\_jet\_num \leq 1$
    \item[$PRI\_jet\_subleading\_phi$] undefined if $PRI\_jet\_num \leq 1$
    \item[$PRI\_jet\_subleading\_eta$] undefined if $PRI\_jet\_num \leq 1$
    \item[$DER\_lep\_eta\_entrality$] undefined if $PRI\_jet\_num \leq 1$
    \item[$PRI\_jet\_leading\_pt$] undefined if $PRI\_jet\_num = 0$
    \item[$PRI\_jet\_leading\_eta$] undefined if $PRI\_jet\_num = 0$
    \item[$PRI\_jet\_leading\_phi$] undefined if $PRI\_jet\_num = 0$   
\end{description}

So we can see that the missing data is explained by the combination of an unexpected topology and the number of jets observed. This implies that the physical data generating process is different in each of these regimes and that we ought to treat them differently in accordance. As such we chose to attempt to build a separate model for each regime. 

\section{Method}
\label{method}

\subsection{Performance Metrics}

\subsubsection{ROC and AUC}
A ROC (Receiver Operating Characteristic) curve plots the true positive rate against the false positive rate for a binary classifier with a varying decision threshold. By doing this we can show the ability of the classifier to dscriminate between the two classes and attain a metric for the performance that we can use to compare models. The worst possible model is given by a line at $y=x$ and any curve above this line is an improvement. 

The AUC (Area Under the [ROC] Curve) provides a compression of the assessment the ROC curve provides by integrating the ROC curve and giving a value in $[0,1]$ that represents an overall view of the discriminatory power of the model. A value of 0.5 corresponds to the worst model ($y=x$) whilst 1.0 would represent a perfect model. 

Note that for our problem, the actual ratio of signal:background events is very small, which would cause a problem in interpreting the AUC reliably. In the actual dataset, this problem is mitigated by providing comparable numbers of rows with signal labels as to rows with background labels, each with an associated weight, such that the overall weighted events of signal and background match the actual observed data.

\subsubsection{AMS}
The AMS metric is the Approximate Median discovery Significance;  an approximation of the \emph{significance} defined by 
\eq{
    Z &= \Phi^{-1}(1-p) \\
    &= \sqrt{q_0} \\
    &= \sqrt{2\left(n\ln\frac{n}{\mu_b} - n + \mu_b\right)}
}
where $n$ is the (unknown) number of events in some search region $\mc{G}$, $\Phi^{-1}$ is the inverse Normal CDF, $q_0$ is a test statistic given by Wilks' Theorem and $\mu_b$ is the (unknown) expected number of background events. We replace $n\rightarrow s+b$ and $\mu_b\rightarrow b$ with $s,b$ the estimator of the expected number of signal and background events respectively.

See \cite{HiggsML} for more details.

Since this is the formal objective we aim to maximise, it might make sense to try and optimise it directly. A little analysis however reveals that the function is non-convex and therefore a poor choice. See \ref{appendix:ams} for the full calculation.

To compute the AMS for a given model output $p \in [0,1]$, we also need to define a \emph{decision rule} that determines the threshold probability at which a signal is declared. For example, if we choose a default threshold, with no prior knowledge at $\theta = 0.5$ then we set our prediction $\hat{y} = s$ if $p > \theta$. In much the same way as we plot the ROC curve by omputing the TPR and FPR at different thresholds, so we can plot a curve of AMS against this threshold and thereby determine an optimal value. 

\subsection{Logistic Regression}
We chose to use a logistic regression as a baseline model to try and solve our classification task. The logic for this was two-fold; we would have a robust, flexible model to compare other modelling approaches to, as well as retaining the capacity to augment it with additional features which could include other models as sub-models within the logistic regression model. 

\subsection{SVM}

\subsection{Feature Engineering}
In order to improve the performance of our model, we attempted to carry out some feature engineering to extract as much information as possible from the dataset.

\subsubsection{Redundancy}
By considering the basic physics of the beam it is possible to see that there is some redundancy amongst the (primitive) features in the dataset. We can exploit this fact and reduce our initial feature space slightly, by transforming the redundant features into a set of new derived features that contain the same information. 
More specifically, the redundancy comes from the consideration that the physical phenomena should be invariant to certain symmetries; a rotation about the beam ($z$) axis and reflections in the $xy$-plane.

In particular, we defined the following new features:
\eq{
    PRI\_lep\_phi-PRI\_tau\_phi &= (PRI\_lep\_phi - PRI\_tau\_phi) \mod{2\pi} \\
    PRI\_met\_phi-PRI\_tau\_phi &= (PRI\_met\_phi - PRI\_tau\_phi) \mod{2\pi} \\
    PRI\_jet\_leading\_phi-PRI\_tau\_phi &= (PRI\_jet\_leading\_phi - PRI\_tau\_phi) \mod{2\pi} \\
    PRI\_jet\_subleading\_phi-PRI\_tau\_phi &= (PRI\_jet\_subleading\_phi - PRI\_tau\_phi) \mod{2\pi} \\
    PRI\_tau\_eta &= \rm{sign}(PRI\_tau\_eta)PRI\_tau\_eta \\
    PRI\_lep\_eta &= \rm{sign}(PRI\_tau\_eta)PRI\_lep\_eta \\
    PRI\_jet\_leading\_eta &= \rm{sign}(PRI\_tau\_eta)PRI\_jet\_leading\_eta \\
    PRI\_jet\_subleading\_eta &= \rm{sign}(PRI\_tau\_eta)PRI\_jet\_subleading\_eta
}

\subsubsection{Higher-Order Effects}
From the exploratory analysis of the data, in particular the visualisation of the principal components, it appears that the classes are not linearly separable in the base feature space. As a result, we hope that perhaps by including non-linear transformations of our features, such as polynomials and RBF centroids, we might be able to model some of the non-linear elements of the relationship between the classes and the features.

\paragraph{RBF Centroids}
To attempt to capture generic higher order interaction-type effects, we implemented a set of RBF centroid features. The implementation works as follows \newline
\begin{algorithm}[H]
    1. Generate heuristic estimate of RBF hyperparameter $s$.\\
    2. Select $n_c$ points from $\Xv_{train}$, $\Xv_c = \{\xv^{train}_i, \dots, \xv^{train}_{n_c}\}$.\\
    3. \For{i=1:$n_c$}{
        Calculate $\kv_{RBF}(\xv^{train}_i, \Xv_{train}; s)$\\
        $\Xv_{train} \leftarrow \Xv_{train} \oplus \kv_{RBF}(\xv^{train}_i, \Xv_{train}; s)$
    } 
    4. Fit model.\\
    5. \For{i=1:$n_c$}{
        Calculate $\kv_{RBF}(\xv^{train}_i, \Xv_{test}; s)$\\
        $\Xv_{test} \leftarrow \Xv_{test} \oplus \kv_{RBF}(\xv^{train}_i, \Xv_{test}; s)$
    }
    \caption{Augment covariate matrix with RBF centroid features}
\end{algorithm}

\subsubsection{Polynomial Transformations}
In order to try and model non-linear relationships between our features and the target labels, we decided to try and include polynomial transformations of the features. 

\subsubsection{Interactions}
Although we considered implementing interactions to further augment our covariates, we chose not to pursue this avenue in the end. The reason for this is that the number of combinations of pairwise interactions for our feature set is 378 Including all of these is likely to significantly increase the risk of overfitting our model, and without a principled method to either choose to only include a subset of interactions, or a way to remove most of them, it seems preferable to skip this option. Additionally, the large increase in feature space would also lead to a sizeable increase in computation time, as the Hessian dimensions would increase by a factor $\sim 4-5$ and hence the time by approximately two orders of magnitude (as our algorithm uses Newton's method which scales as $O(d^3)$ for a $d$-dimensional feature space) for the case where we include polynomial terms upto third order. If we include only the original features plus interactions, the computation time scales by worse than three orders of magnitude in comparison.

\section{Results}
\label{results}

\subsection{Model Selection}

\subsubsection{Cross Validation}
To make decisions regarding the relative performance of our candidate models we used a 10-fold cross-validation procedure to train and test each of the permutations of our features and models. By running experiments that carried out the procedure, we were able to generate summary outputs that concisely captured a representation of performance. More precisely, we appended rows of information to a $.csv$ file for each experiment that could then be subsequently analysed to compare performance. Note that each experiment was run with an unspecified random seed, so we expect some degree of random variation between experiments even with no change to parameters, simply due to the change in data partitioning. We can use this scale of variance to aid identification of significant performance improvements. 10 fold cross-validation was picked as a compromise between speed of computation and model variance; computation time scales as $O(KNd)$ for $K$ folds, $N$ data points and $d$ features, whilst we expect the variance of our (averaged over folds) parameter estimates to scale as $\frac{1}{K}$. In effect, by picking (for example) 10 folds rather than 5, we are trading an increase in computation time of a factor of two for a decrease in model variance of the same factor, for an equivalent variance in our performance estimate.

\subsubsection{AMS Threshold}
Part of our model selection process is to determine the thresholds we want to choose as part of our decision rule to convert probabilistic model outputs into class (signal) predictions. By viewing the curves of AMS against threshold for each fold, on each model category, we can see how consistent this relationship is.

\input{results_table}
\label{table:1}

\subsection{Predictions}
To actually calculate AMS scores, we need to convert out probabilistic outputs into binary labels which means we need to pick a decision rule, i.e. a threshold over which we assign a label 1 (or $s$) vs 0 (or $b$).

Using the model that ranked the best in terms of scaled AMS, we get the following results on the cross-validated out-of-sample performance for the training set and the hold-out test set:
\newline
Results for $\lambda=0.0001$, $G=3$, $n_{rbf}=2$, $K=10$ on training set ('t') and validation set ('v'):
\begin{description}
    \item[CV OOS AUC:] $0.87 \pm 0.0$
    \item[CV OOS AMS:] $2.43 \pm 0.9$
    \item[Test set AUC:] $0.85 \pm 0.0$
    \item[Test set AMS:] $1.97 \pm 1.2$
\end{description} 
where the "error" term here shows the mean absolute deviation across folds.

Clearly the result for the test set is significantly worse than that for the 10-fold out-of-sample measure. A possible explanation can be found by looking at the AMS threshold curves for the two datasets for this model. For the group corresponding to 0 jets, the threshold for the maximum AMS is substantially lower than what we saw in the training set at 0.2 compared to 0.6. The other two jet categories have an optimal threshold of 0.5 in the test set, versus 0.4 and 0.6 in the training set. 

\begin{figure}[h]
    \includegraphics[width=8cm]{figs/validation-ams-curves.pdf}
\end{figure}

\section{Conclusion}
\label{conclusion}

\pagebreak 

\input{lhc_appendix}
\bibliography{lhc.bib}{}
\bibliographystyle{plain}

\end{document}