\input{compass_preamble}

\usepackage[toc,page]{appendix}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{pifont}
% \setlist{nolistsep,leftmargin=*}

\graphicspath{{./figs/}}

\pagestyle{fancy}
\rhead{}
\lhead{}

\author{Georgina Mansell and Anthony Stephenson}
\date{}
\title{Identifying fermionic decay signals of the Higgs boson with classification algorithms}

\begin{document}
\maketitle
\begin{abstract}
    Analysing the fermionic Higgs decay channel $H\rightarrow\tau\tau$ is important for providing experimental evidence of the Higgs mechanism for fermions. Using statistical classification algorithms we aim to optimise the signal:background ratio in the data (measured by the AMS). In particular, we build a logistic regression model with some engineered features augmenting the original dataset. Using 10-fold cross-validation we select select a model with the best AMS and then test its performance on a hold-out test set. On this set we achieve an AMS of 2.0.
\end{abstract}

\section{Introduction}
After the theoretical discovery of the Higgs mechanism and the associated particle, the Higgs boson, experimental confirmation of the prediction became a much sought-after prize in High Energy Physics. Although the theory was more or less accepted by the theoretical community since its inception in the 1960s, experimental evidence was required for verification. 

In 2010 CERN's Large Hadron Collider (LHC) achieved the necessary power to probe higher energy regimes of collisions and since then has collected vast amounts of data. In order to verify the theoretical predictions of particle physics, this data needed to be anaylsed carefully to try and distinguish signals from an unknown particle decays from other known decays. The accepted threshold for a discovery in particle physics is the ``gold-standard'' 5-sigma rule. As a result, experimental discovery of the Higgs was required to meet this threshold. 

Various decay mechanisms of the Higgs boson were proposed that could be used to demonstrate its existence. In 2013, when the Higgs was experimentally discovered by CERN, the evidence was provided by \emph{bosonic} decays from the Higgs to the following pairs, $\gamma\gamma$, $WW$ and $ZZ$. The goal here is to instead examine the coupling of the Higgs to fermions, to verify that their mass can likewise be explained by the Higgs mechanism. Specifically, the aim is to analyse the decay of the Higgs to a pair of tau-flavour leptons $H\rightarrow\tau\tau$.

In this document we implement classification models to identify signal events $H\rightarrow\tau\tau$ from other background events. An overview of the data set is provided in \ref{data}, the classification techniques and performance metrics are introduced in \ref{method}, candidate models are compared in \ref{results}, and the key findings and validation results are summarised in \ref{conclusion}.

\section{Data}
\label{data}
\subsection{Structure}
The dataset was provided by CERN for a Machine Learning competition on the website Kaggle. It contains roughly 800,000 proton-proton collision events from the official ATLAS detector simulator. It includes 16 primitive variables measured by the detector, 13 variables derived from the primitive variables, the event classifications, and 4 columns of auxiliary data such as sample weights. 

\subsection{Class Imbalance}
Signal events occur in approximately 1/1000 collisions. If events in the data set were na\"{\i}vely simulated at their true rate, this would create a highly imbalanced dataset. Classification algorithms can often be modified to account for class imbalance, however as the CERN dataset was simulated the number of samples in each class could be controlled. In the generated dataset approximately 1/3 samples are classified as signal events and the samples are given importance weightings corresponding to their true rate of occurrence. The associated weights of signal and background samples are roughly $O(10^{-3})$ and $O(1)$.

\subsection{Missing Data}
Early exploration of the dataset revealed the presence of a significant quantity of missing data. Elements of the data matrix recorded as missing were labelled by a value of -999. Before embarking on any model building or even feature engineering we analysed the missing values to try and ascertain whether there was any pattern to their locations. From the definitions of covariates in the dataset it was possible to infer potential causes of missing data. In particular, the estimated mass of the Higgs ($DER\_mass\_MMC$) ``may be undefined if the topology of the event is too far from the expected topology". Additionally:
\begin{itemize}
	\item If $PRI\_jet\_num = 1$ the following values are undefined:
	$DER\_deltaeta\_jet\_jet$, $DER\_mass\_jet\_jet$, $DER\_prodeta\_jet\_jet$, $DER\_lep\_eta\_centrality$, $PRI\_jet\_subleading\_pt$, $PRI\_jet\_subleading\_eta$, $PRI\_jet\_subleading\_phi$
    \item If $PRI\_jet\_num = 0$ the additional values are undefined: 
    $PRI\_jet\_leading\_pt$, $PRI\_jet\_leading\_eta$, $PRI\_jet\_leading\_phi$
\end{itemize}

Missing values are therefore not random, and are explained by the combination of an unexpected topology and the number of jets observed. This implies that the physical process is sufficiently different in these regimes that they should be modelled differently in accordance. 

\section{Method}
\label{method}

\subsection{Performance Metrics}

\subsubsection{ROC and AUC}
A ROC (Receiver Operating Characteristic) curve plots the true positive rate against the false positive rate for a binary classifier with a varying decision threshold. By doing this we can show the ability of the classifier to dscriminate between the two classes and attain a metric for the performance that we can use to compare models. Chance performance is given by a line at $y=x$ and any curve above this line is an improvement. 

The AUC (Area Under the [ROC] Curve) provides a compression of the assessment the ROC curve provides by integrating the ROC curve and giving a value in $[0,1]$ that represents an overall view of the discriminatory power of the model. A value of 0.5 corresponds to the worst model ($y=x$) whilst 1.0 would represent a perfect model. 

For this classification problem, the number of samples of each class do not represent the true ratio of measurements. This means that AUC does not provide a full picture of the model performance, since it only considers the number of true and false positives and does not take into account the samples weights. Instead, the Kaggle competition propose AMS as the objective metric.

\subsubsection{AMS}
The AMS metric is the Approximate Median discovery Significance;  an approximation of the \emph{significance} defined by 
\eq{
    Z &= \Phi^{-1}(1-p) \\
    &= \sqrt{q_0} \\
    &= \sqrt{2\left(n\ln\frac{n}{\mu_b} - n + \mu_b\right)}
}
where $n$ is the (unknown) number of events in some search region $\mc{G}$, $\Phi^{-1}$ is the inverse Normal CDF, $q_0$ is a test statistic given by Wilks' Theorem and $\mu_b$ is the (unknown) expected number of background events. We replace $n\rightarrow s+b$ and $\mu_b\rightarrow b$ with $s,b$ the estimator of the expected number of signal and background events respectively.

See \cite{HiggsML} for more details.

Since this is the formal objective we aim to maximise, it might make sense to try and optimise it directly. A little analysis however reveals that the function is non-convex and therefore a poor choice. See \ref{appendix:ams} for the full calculation.

To compute the AMS for a given model output $p \in [0,1]$, we also need to define a \emph{decision rule} that determines the threshold probability at which a signal is declared. For example, if we choose a default threshold, with no prior knowledge at $\theta = 0.5$ then we set our prediction $\hat{y} = s$ if $p > \theta$. In much the same way as we plot the ROC curve by computing the TPR and FPR at different thresholds, so we can plot a curve of AMS against this threshold and thereby determine an optimal value. 

\subsection{SVM}
We considered trying to use Support Vector Machines as a classification algorithm, and created our own implementation of the algorithm in R. While standard SVMs find an optimal linear decision boundary, kernel SVM can project the data into a higher dimensional space and find a more complex boundary. We discuss the Lagrangian optimisation problem for both soft-margin and kernel svm in \ref{app:svm}. Unfortunately, SVM was not well suited to this classification problem as the two classes are not well separated. Additionally, this is a large dataset and SVM scales poorly with the number of training samples, and so we are only able to train on a fraction of the data.

Ensemble SVM is a technique where multiple SVM models are trained on subsets of the training data. To classify new samples, the majority vote from the component models is taken. Dividing a dataset of $n$ samples over $m$ models reduces the computational complexity of an $\mathcal{O}(n^2)$ algorithm to $\mathcal{O}(n^2/m)$. In Figure \ref{fig:svm}, we trained 10 soft margin SVM models, each on 500 samples, and classified the remaining samples in each group. We show that the ensemble models outperforms the component models. 

\begin{figure}[h]
    \centering
	\includegraphics[width=0.5\textwidth]{Ensemble_SVM.pdf}
    \caption{Performance of 10 SVM models and an ensemble SVM for each jet group}
    \label{fig:svm}
\end{figure}

\subsection{Logistic Regression}
We chose to use a logistic regression as a baseline model to try and solve our classification task. The logic for this was two-fold; we would have a robust, flexible model to compare other modelling approaches to, as well as retaining the capacity to augment it with additional features which could include other models as sub-models within the logistic regression model. 

\subsubsection{Implementation}
We implemented our logistic regression model by defining an R class with a standard interface (to enable polymorphism) and an associated fitting function. This function made use of Newton's method (\ref{algo:newton}) with a backtracking linesearch (\ref{algo:line}) in order to calculate maximum likelihood estimates for our model coefficients, $\betav$. \ref{appendix:newton_lr} show's the full calculations of the gradient and Hessian of the logistic likelihood function required to implement Newton's method.

\subsection{Feature Engineering}
In order to improve model performance of our model, the following feature engineering was conducted.

\subsubsection{Redundancy}
By considering the basic physics of the beam it is possible to see that there is some redundancy amongst the primitive features in the dataset. We can exploit this fact and reduce our initial feature space slightly, by transforming the redundant features into a set of new derived features that contain the same information. 
More specifically, the redundancy comes from the consideration that the physical phenomena should be invariant to certain symmetries; a rotation about the beam ($z$) axis and reflections in the $xy$-plane.

In particular, we defined the following new features:
\eq{
    PRI\_lep\_phi-PRI\_tau\_phi &:= (PRI\_lep\_phi - PRI\_tau\_phi) \mod{2\pi} \\
    PRI\_met\_phi-PRI\_tau\_phi &:= (PRI\_met\_phi - PRI\_tau\_phi) \mod{2\pi} \\
    PRI\_jet\_leading\_phi-PRI\_tau\_phi &:= (PRI\_jet\_leading\_phi - PRI\_tau\_phi) \mod{2\pi} \\
    PRI\_jet\_subleading\_phi-PRI\_tau\_phi &:= (PRI\_jet\_subleading\_phi - PRI\_tau\_phi) \mod{2\pi} \\
    PRI\_tau\_eta &:= \rm{sign}(PRI\_tau\_eta)PRI\_tau\_eta \\
    PRI\_lep\_eta &:= \rm{sign}(PRI\_tau\_eta)PRI\_lep\_eta \\
    PRI\_jet\_leading\_eta &:= \rm{sign}(PRI\_tau\_eta)PRI\_jet\_leading\_eta \\
    PRI\_jet\_subleading\_eta &:= \rm{sign}(PRI\_tau\_eta)PRI\_jet\_subleading\_eta
}

\subsubsection{Higher-Order Effects}
From the exploratory analysis of the data, in particular the visualisation of the principal components, it appears that the classes are not linearly separable in the base feature space. As a result, we hope that including non-linear transformations of our features, such as polynomials and RBF centroids, will help model the complex relationship between the classes and the features.

\subsubsection{RBF Centroids}
To attempt to capture generic higher order interaction-type effects, we implemented a set of RBF centroid features. The implementation works as follows: \newline
\begin{algorithm}[H]
    1. Generate heuristic estimate of RBF hyperparameter $s$.\\
    2. Select $n_c$ points from $\Xv_{train}$, $\Xv_c = \{\xv^{train}_i, \dots, \xv^{train}_{n_c}\}$.\\
    3. \For{i=1:$n_c$}{
        Calculate $\kv_{RBF}(\xv^{train}_i, \Xv_{train}; s)$\\
        $\Xv_{train} \leftarrow \Xv_{train} \oplus \kv_{RBF}(\xv^{train}_i, \Xv_{train}; s)$
    } 
    4. Fit model.\\
    5. \For{i=1:$n_c$}{
        Calculate $\kv_{RBF}(\xv^{train}_i, \Xv_{test}; s)$\\
        $\Xv_{test} \leftarrow \Xv_{test} \oplus \kv_{RBF}(\xv^{train}_i, \Xv_{test}; s)$
    }
    \caption{Augment covariate matrix with RBF centroid features}
\end{algorithm}

\subsubsection{Polynomial Transformations}
We testing including polynomial transformations of the features. We denote the (highest) order of the polynomial transformation included in the model by the variable $b \in \{1,2,3\}$. 

Interaction terms were not included due to the number of features in the dataset. Including just first-order interactions would create an additional ${28 \choose 2} = 378$ features. Including all these variables will significantly increase the risk of over fitting. Additionally, the large increase in feature space would also lead to a sizeable increase in computation time, as the Hessian dimensions would increase by a factor $\sim 4-5$ and hence the time by approximately two orders of magnitude (as our algorithm uses Newton's method which scales as $O(d^3)$ for a $d$-dimensional feature space) for the case where we include polynomial terms upto third order. If we include only the original features plus interactions, the computation time scales by worse than three orders of magnitude in comparison.

\section{Results}
\label{results}

\subsection{Cross Validation}
To make decisions regarding the relative performance of our candidate models we used a 5-fold cross-validation procedure to train and test permutations of hyperparameters. By running experiments that carried out the procedure, we were able to generate summary outputs that concisely captured a representation of performance. More precisely, we appended rows of information to a $.csv$ file for each experiment that could then be subsequently analysed to compare performance. Note that each experiment was run with an unspecified random seed, so we expect some degree of random variation between experiments even with no change to parameters, simply due to the change in data partitioning. We can use this scale of variance to aid identification of significant performance improvements. 10 fold cross-validation was picked as a compromise between speed of computation and model variance; computation time scales as $O(KNd)$ for $K$ folds, $N$ data points and $d$ features, whilst we expect the variance of our (averaged over folds) parameter estimates to scale as $\frac{1}{K}$. In effect, by picking (for example) 10 folds rather than 5, we are trading an increase in computation time of a factor of two for a decrease in model variance of the same factor, for an equivalent variance in our performance estimate.

\subsection{AMS Threshold}
Part of our model selection process is to determine the thresholds we want to choose as part of our decision rule to convert probabilistic model outputs into class (signal) predictions. By viewing the curves of AMS against threshold for each fold (\ref{fig:ams_curve}), on each model category, we can see how consistent this relationship is. All three model types show a fairly high amount of variance between folds, although $j=2+$ appears to be the noisiest. There does appear to be a reasonable degree of agreement between curves however. In order to try and select optimal thresholds, we decided to calculate the minimum over the curves at each threshold, to get an effective lower bounding curve and then find the maximum of this, as a conservative estimate that we hope will minimise the chance of overfitting. 
A few experiments over different models (varying parameters including $n_{rbf}$ and inclusion of polynomial transformations) we do see that the optimal thresholds are model dependent, which adds to the difficulty. To remove this obstacle, we chose to``integrate out" the threshold as a parameter, by averaging over the results computed over a range of thresholds, in effect, integrating each AMS curve, normalising, and averaging the results.


\begin{figure}[htbp]
    \begin{adjustbox}{max width=1.4\linewidth,center}
    \centering
	\includegraphics[width=1.2\textwidth]{CV_LogReg_AMS}
    \end{adjustbox}
    \caption{10-fold cross validation of top model for each jet category}
    \label{fig:ams_curve}
\end{figure}


\subsection{Model Selection}
To finally choose our best performing model, we needed to define the criterion with which we want to judge their success. For each experiment, we recorded both the average and mean absolute deviation of the AUC and AMS. After some consideration, we concluded that since our goal was ultimately to maximise AMS, that ought to form the primary benchmark, although experimental results for some models showed a high degree of variability between folds. We considered scaling the AMS measurements by the mean absolute deviation, to try and optimise for the most consistently effective model, though the top performing models in terms of AMS were not too variable. The top model for each group is shown in table~\ref{table:results}. 

\input{results_table_groups}
To try and determine how reliable our performance assessment might be, we plotted some figures for both performance measures (AUC and AMS) against varying regularisation, numbers of RBF centroids and order of polynomials. Restricting this summary analysis to our preferred metric, the unscaled AMS, we see the results in the boxplots in figure~\ref{fig:boxes}. The AMS results do not change much as $n_{rbf}$ and $\lambda$ are varied, though including polynomial transformations of the variables show a decisive improvement. These figures are consistent with the results in table~\ref{table:results} and so we opted for the final set of models with the following parameters:
\eq{
    \bm{\lambda} &= (0.00464, 1\times 10^{-4}, 10) \\
    \bm{b} &= (3, 3, 3) \\
    \bm{n_{rbf}} &= (1, 0, 1) \\
    \bm{\theta} &= (0.6, 0.4, 0.7)
}
for regularisation parameter $\boldsymbol{\lambda}=(\lambda_{j=0}, \lambda_{j=1}, \lambda_{j=2+})$, polynomial order $\bm{b}=(b_{j=0}, b_{j=1}, b_{j=2+})$, number of RBF centroids $\bm{n_{rbf}}=(n_{rbf,j=0}, n_{rbf,j=1}, n_{rbf,j=2+})$ and vector of jet group AMS thresholds $\bm{\theta} = (\theta_{j=0}, \theta_{j=1}, \theta_{j=2+})$.

\begin{figure}[h]
    \centering
    \subfloat[$n_{rbf}$]{
        \includegraphics[width=0.5\textwidth] {Experiments_nrbf.pdf}
        \label{fig:boxrbf}} 
    \subfloat[Polynomial order]{
        \includegraphics[width=0.5\textwidth] {Experiments_poly.pdf}
        \label{fig:boxpoly} } 
       
    \subfloat[L2 regularisation]{
        \includegraphics[width=0.5\textwidth] {Experiments_lambda.pdf}
        \label{fig:boxlambda} }
        
    \caption{Box plots of experimental results varying parameters/features}
    \label{fig:boxes}
\end{figure}

\subsection{Predictions}
To actually calculate AMS scores, we need to convert out probabilistic outputs into binary labels which means we need to pick a decision rule, i.e. a threshold over which we assign a label 1 (or $s$) vs 0 (or $b$). Since we have a different model per jet group, we choose a different optimal threshold per model type and calculate our overall result by computing the average AMS over each group (applying its threshold) and averaging the results. Restating the same thing in more mathematical notation, we compute 
\eq{
    \langle \rm{AMS} \rangle &= \frac{1}{G}\sum_{g=1}^G\frac{1}{K}\sum_{k=1}^K \rm{ams}(g, k, \theta_g)
}
with jet groups $g$, folds $k$ and threshold for group $g$, $\theta_g$ and a function $\rm{ams}(g, k, \theta_g)$ which computes the AMS for a particular group-fold combination given its threshold.

Using the model that ranked the best in terms of scaled AMS, we get the following results on the cross-validated out-of-sample performance for the training set and the hold-out test set are in table~\ref{table:final_results}. We have a final test AUC of 0.833 and when we apply our selected AMS thresholds to samples of each jet group, we achive a final AMS score of 1.998.
\input{final_results_table.tex}


\section{Conclusion}
\label{conclusion}
In this document we present a classification algorithm to detect events in the LHC where a Higgs boson decays into two tau particles. The samples appeared to be generated from three distinct physical processes, so we divided the dataset into three groups based on the recorded number of jets, and then trained a separate classification model for each group. Though we explored SVM, our final models were L2-constrained logistic regression, with RBF centroid features and polynomial transformed features. The hyperparameters for each model $b, n_{rbf}$ and $\lambda$ were selected by performing cross validation over different permutations. On a hold-out test set, we achieved an overall AMS metric of 2.0. 

\pagebreak 

\input{lhc_appendix}
\bibliography{lhc.bib}{}
\bibliographystyle{plain}

\end{document}