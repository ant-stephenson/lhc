---
title: "Feature selection"
author: "Anthony Stephenson"
date: "1/3/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load packages
```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(Matrix)
library(fs)
library(purrr)

source("utility_funcs.r")
source("plot_funcs.r")
source("lr_funcs.r")
source("lr1_funcs.r")
```

Load the data
```{r load data}
# divide data into training kaggle set, and retain hold-out (before further cross-validation partitioning)
source_path <- path_join(c(path_dir(path_dir(getwd()))))
filename <- "atlas-higgs-challenge-2014-v2.csv"
filepath <- path_join(c(source_path, "LHC_dump", "R", filename))
data <- import_data(filepath)
```

Kaggle sets:  ”t”:training, ”b”:public leaderboard, ”v”:private leaderboard, ”u”:unused.
```{r training set}
train_label = c("t")
train_idx <- get_subset_idx(data$kaggle_s, train_label)
X <- data$X[train_idx, ]
y <- data$y[train_idx]
# need to use kaggle weignts to make AMS correct?!
kaggle_w <- data$kaggle_w[train_idx]
w <- data$kaggle_w[train_idx]
nj <- data$nj[train_idx]
e_id <- data$e_id[train_idx]
```

```{r validation set}
# public leaderboard set
val_label <- c("b")
val_idx <- get_subset_idx(data$kaggle_s, val_label)
Xv <- data$X[val_idx, ]
yv<- data$y[val_idx]
wv <- data$kaggle_w[val_idx]
njv <- data$nj[val_idx]
```

Apply some feature engineering
```{r add features}
# modify features
X <- reduce_features(X)
X <- invert_angle_sign(X)
Xv <- reduce_features(Xv)
Xv <- invert_angle_sign(Xv)

# X <- poly_transform(X, 2)
# Xv <- poly_transform(Xv, 2)

# ensure X and Xv have the same columns
cols2keep <- setdiff(colnames(Xv), setdiff(colnames(Xv), colnames(X)))
Xv <- Xv[, cols2keep]

Xv <- scale_dat(Xv, X, na.rm=TRUE)
```

Initialise parameters
```{r init params}
n_rbf <- 0
s <- avg_median_pairwise_distance(X)

# Regularisation (L2) parameter [global]
lambda <- 1e-3

# constraint parameter
c <- 1

# K-Fold CV partitioning
K <- 10
kI <- partition_data(length(y), K, random = TRUE)

n <- nrow(X)
d <- ncol(X) + n_rbf

# number of jet/Higgs mass groups to build different models for
G <- 3
thresholds <- c(0.7, 0.4, 0.8)

# choose model
# logreg
# model_init <- partial(logistic_model$new, lambda=lambda)
rm(logistic)
model_init <- partial(logistic_l1_model$new, c=c)

#
result_label <- "_l1_logreg_"

sum_w <- sum(w)
```


Use missing data pattern to partition data
```{r jet/missing}
# get missing rows. separate by number of jets and presence of Higgs mass and fit separate models
# find columns with features with any missing values for each number of jets: 0, 1, 2+ in combination with the presence (or absence) of the Higgs mass, defined by j=1,2,3, 4, 5, 6. i.e. j=1 => nj=0 & mH != -999, j=2 => 
jet_cats <- c(1:3, 1:3)
features_to_rm <- set_features_to_rm(X, G, kI, nj)
```

```{r test, include=FALSE}
midx <- matrix(, nrow=G*K,ncol=2)
  for (j in 1:G) {
    for (k in 1:K) {
      idx <- get_model_idx(j, k, K)
      midx[idx,] <- inv_model_idx(idx, K)
    }
  }
midx
```

Count the number of rows of data in each category and fold, with the first column being the training set and the second being the test for each fold/category
```{r number of rows}
n_rows_p_partition <- matrix(, nrow=G*K, ncol=2)
for (mj in 1:G) {
    # loop over sets of jet number {0, 1, 2+} and mH presence/absence
    for (k in 1:K) {
        j <- jet_cats[mj]
        fit_row_idx <- kI != k & idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)
        test_row_idx <- kI == k & idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)

        model_idx <- get_model_idx(mj, k, K)

        n_rows_p_partition[model_idx, 1] <- sum(fit_row_idx)
        n_rows_p_partition[model_idx, 2] <- sum(test_row_idx)
    }
}
n_rows_p_partition
```

Run models
```{r fitting}
# loop over folds
#create lists to hold the k models and k roc curves
models <- vector("list", G*K)
rocs <- vector("list", G*K)
ams_obj <- vector("list", G*K)
b <- matrix(, nrow=d, ncol=G*K)

ams <- rep(NA, length=G*K)
auc <- rep(NA, length=G*K)

par(mfrow=c(2, 3))
for (mj in 1:G) {
    # loop over sets of jet number {0, 1, 2+} and mH presence/absence
    for (k in 1:K) {
        j <- jet_cats[mj]
        fit_row_idx <- kI != k & idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)
        test_row_idx <- kI == k & idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)

        # add r RBF centroid features, using the same reference centroids in training and testing sets
        if (n_rbf > 0) {
          rbf_centroids <- get_rbf_centroids(X[fit_row_idx, ], n_rbf)
          Xi <- rbf_centroids$"xi"
          Xtrain <- add_rbf_features(X[fit_row_idx, ], s, n_rbf, Xi=Xi)
          Xtest <- add_rbf_features(X[test_row_idx, ], s, n_rbf, Xi=Xi)
        } else {
          Xtrain <- X[fit_row_idx, ]
          Xtest <- X[test_row_idx, ]
        }
        
        col_idx <- get_valid_cols(colnames(X), features_to_rm, mj)

        Xtrain <- as.matrix(Xtrain[, col_idx])
        Xtest <- as.matrix(Xtest[, col_idx])
        
        Xtest <- scale_dat(Xtest, Xtrain)
        Xtrain <- scale_dat(Xtrain, Xtrain)

        model_idx <- get_model_idx(mj, k, K)

        #fit a logistic regression model to the CV training data
        models[[model_idx]] <- model_init(X=Xtrain, y=y[fit_row_idx])
  
        #use it to predict the classifications of the test data
        p_hat <- models[[model_idx]]$predict(Xtest)

        #create an ROC curve object
        rocs[[model_idx]] <- ROC_curve$new(y[test_row_idx], p_hat)
        
        ams_obj[[model_idx]] <- AMS_data$new(y[test_row_idx], p_hat, w[test_row_idx], sum_w=sum_w)
    }
}
```
```{r avg auc and plot roc, error=TRUE}
auc <- sapply(rocs, function(x) x$calc_auc())
for(j in 1:G){
  #find average auc over folds
  i1 <- get_model_idx(j, 1, K)
  i2 <- get_model_idx(j, K, K)
  average_auc <- mean(sapply(rocs[i1:i2], function(x) x$auc), na.rm=TRUE)
  #get a plot of the k ROC curves on the same axis
  rocs[[i1]]$plot_curve(title=sprintf("All training data, %i-fold CV, Average AUC %.3f", K, round(average_auc, 3)))
  for (k in 2:K) {
    rocs[[get_model_idx(j, k, K)]]$plot_curve(add=T)
  }
}

```
If we now also plot curves of the AMS against threshold we can get a feeling for what we should choose and how consistent they are across folds (for each model type). They are all relatively noisy, although `j=3` is comfortably the worst of the three. Although the threshold that gives the maximum threshold is averaged across the folds (and that is what is listed in the legend) the results are quite noisy, and rerunning the analysis with randomly refolded data gives differences in the region of $10^{-2}$, so we don't want to overfit our threshold choice. On this logic, picking $(0.7, 0.4, 0.8)$ seems reasonable.
```{r avg ams and plot roc, error=TRUE}
ams <- sapply(ams_obj, function(x) x$calc_ams())
for(j in 1:G){
  #find average auc over folds
  i1 <- get_model_idx(j, 1, K)
  i2 <- get_model_idx(j, K, K)
  average_max_thresh <- mean(sapply(ams_obj[i1:i2], function(x) x$get_max_thresh()), na.rm=TRUE)
  #get a plot of the k ROC curves on the same axis
  ams_obj[[i1]]$plot_ams(lgd=paste0("Average Max. AMS at p=", round(average_max_thresh, 2)))
  for (k in 2:K) {
    ams_obj[[get_model_idx(j, k, K)]]$plot_ams(add=T)
  }
}
```

Fit model (for each category) on whole (unfolded) dataset
```{r full model}
# loop over folds
#create lists to hold the k models and k roc curves
full_models <- vector("list", G)
full_rocs <- vector("list", G)
rbf_idx <- matrix(, nrow=G, ncol=n_rbf)
# check warnings?
for (mj in 1:G) {
    # loop over sets of jet number {0, 1, 2+} and mH presence/absence
    j <- jet_cats[mj]
    fit_row_idx <- idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)
        
    if (n_rbf > 0) {
      # add r RBF centroid features, using the same reference centroids in training and testing sets
      rbf_centroids <- get_rbf_centroids(X[fit_row_idx, ], n_rbf)
      Xi <- rbf_centroids$"xi"
      # record which rows to use for OOS in public set (or any other OOS)
      rbf_idx[mj, ] <- rbf_centroids$"idx"
      Xtrain <- add_rbf_features(X[fit_row_idx, ], s, n_rbf, Xi=Xi)
    } else {
          Xtrain <- X[fit_row_idx, ]
    }

    col_idx <- get_valid_cols(colnames(Xtrain), features_to_rm, mj)

    Xtrain <- as.matrix(Xtrain[, col_idx])
    Xtrain <- scale_dat(Xtrain, Xtrain)

    model_idx <- get_model_idx(mj, 1, 1)

    #fit a logistic regression model to the CV training data
    full_models[[model_idx]] <- model_init(X=Xtrain, y=y[fit_row_idx])
}
```

Test on (until now unseen) validation set. Calling it validation as we can then use the summary metrics to check free parameters, being careful not to overdo it.
```{r validation}
amsv_obj <- vector("list", G)

# check warnings
sum_wv <- sum(wv)
for (mj in 1:G) {
    j <- jet_cats[mj]
    test_row_idx <- idx_jet_cat(njv, j) & idx_higgs_mass(Xv, mj, G)

    if (n_rbf > 0) {
      # add r RBF centroid features, using the same reference centroids in training and testing sets
      Xi <- X[rbf_idx[mj, ], ]
      Xtest <- add_rbf_features(Xv[test_row_idx, ], s, n_rbf, Xi=Xi)
    } else {
          Xtest <- Xv[test_row_idx, ]
    }
    
    col_idx <- get_valid_cols(colnames(Xtest), features_to_rm, mj)

    Xtest <- as.matrix(Xtest[, col_idx])

    model_idx <- get_model_idx(mj, 1, 1)

    #use it to predict the classifications of the test data
    p_hat <- full_models[[model_idx]]$predict(Xtest)

    #create an ROC curve object
    full_rocs[[model_idx]] <- ROC_curve$new(yv[test_row_idx], p_hat)

    amsv_obj[[model_idx]] <- AMS_data$new(yv[test_row_idx], p_hat, wv[test_row_idx], sum_w=sum_wv)
}
```

```{r plot validation auc}
. <- sapply(full_rocs, function(x) x$calc_auc())
aucv <- sapply(full_rocs, function(x) round(x$auc, 3))
  #get a plot of the k ROC curves on the same axis
full_rocs[[get_model_idx(1, 1, 1)]]$plot_curve(title=do.call(partial(sprintf, "Validation test-set, OOS AUCs (%.3f,%.3f,%.3f)"), as.list(aucv)))
for(j in 2:G){
  #get a plot of the k ROC curves on the same axis
  full_rocs[[get_model_idx(j, 1, 1)]]$plot_curve(add=T)
}
```
Now plotting the AMS thresholds for the validation set, we see that they differ substantially from those we saw in the training set, despite the K-fold CV:
```{r plot validation ams thresholds}
amsv <- sapply(amsv_obj, function(x) x$calc_ams())
amsv_thresh <- sapply(amsv_obj, function(x) x$get_max_thresh())
max_max_ams <- max(sapply(amsv_obj, function(x) x$max_ams))
colours <- topo.colors(G)
amsv_obj[[1]]$plot_ams(lgd=paste0("Max. AMS at p=", round(amsv_thresh, 2)), add=F, ylim=c(0, max_max_ams), col=colours[1])
for(j in 2:G){
  amsv_obj[[j]]$plot_ams(add=T, col=colours[j])
}
```
Summary results:
```{r print}
sprintf("Results for lambda=%.2g, G=%i, n_rbf=%i, K=%i on training set ('%s') and validation set ('%s')", lambda, G, n_rbf, K, train_label, val_label)
sprintf("CV OOS AUC = %.2f ± %.1f", mean(auc), mad(auc))
sprintf("CV OOS AMS = %.2f ± %.1f", mean(ams), mad(ams))
sprintf("Validation set AUC = %.2f ± %.1f", mean(aucv), mad(aucv))
sprintf("Validation set AMS = %.2f ± %.1f", mean(amsv), mad(amsv))
```

```{r record to file}
result <- list("Train"=train_label, "Validation"=val_label, "K"=K, "G"=G, "n_rbf"=n_rbf, "lambda"=lambda, "auc"=mean(auc), "mad(auc)"=mad(auc), "ams"=mean(ams), "mad(ams)"=mad(ams), "aucv"=mean(aucv), "mad(aucv)"=mad(aucv), "amsv"=mean(amsv), "mad(amsv)"=mad(amsv))
write.table(as.data.frame(result), file = sprintf("results%s.csv", result_label), append = TRUE, sep = ",",
eol = "\n", na = "NA", dec = ".", row.names = FALSE,
col.names = FALSE, qmethod = c("escape", "double"))
```

