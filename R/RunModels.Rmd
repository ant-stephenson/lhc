---
title: "Feature selection"
author: "Anthony Stephenson"
date: "1/3/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load packages
```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(Matrix)
library(fs)
library(purrr)

source("utility_funcs.r")
source("plot_funcs.r")
source("lr_funcs.r")
source("lr1_funcs.r")
```

Load the data
```{r load data}
# divide data into training kaggle set, and retain hold-out (before further cross-validation partitioning)
source_path <- path_join(c(path_dir(path_dir(getwd()))))
filename <- "atlas-higgs-challenge-2014-v2.csv"
filepath <- path_join(c(source_path, "LHC_dump", "R", filename))
data <- import_data(filepath)
```

Kaggle sets:  ”t”:training, ”b”:public leaderboard, ”v”:private leaderboard, ”u”:unused.
```{r training set}
train_label = c("t")
train_idx <- get_subset_idx(data$kaggle_s, train_label)
X <- data$X[train_idx, ]
y <- data$y[train_idx]
# need to use kaggle weignts to make AMS correct?!
kaggle_w <- data$kaggle_w[train_idx]
w <- data$kaggle_w[train_idx]
nj <- data$nj[train_idx]
e_id <- data$e_id[train_idx]
```

```{r validation set}
# public leaderboard set
val_label <- c("b")
val_idx <- get_subset_idx(data$kaggle_s, val_label)
Xv <- data$X[val_idx, ]
yv<- data$y[val_idx]
wv <- data$kaggle_w[val_idx]
njv <- data$nj[val_idx]
```

Apply some feature engineering
```{r add features}
# modify features
X <- reduce_features(X)
X <- invert_angle_sign(X)
Xv <- reduce_features(Xv)
Xv <- invert_angle_sign(Xv)

# X <- poly_transform(X, 2)
# Xv <- poly_transform(Xv, 2)

# ensure X and Xv have the same columns
cols2keep <- setdiff(colnames(Xv), setdiff(colnames(Xv), colnames(X)))
Xv <- Xv[, cols2keep]

Xv <- scale_dat(Xv, X, na.rm=TRUE)
```

Initialise parameters
```{r init params}
n_rbf <- 0
s <- avg_median_pairwise_distance(X)

# Regularisation (L2) parameter [global]
lambda <- 1e-3

# constraint parameter
c <- 1

# K-Fold CV partitioning
K <- 10
kI <- partition_data(length(y), K, random = TRUE)

n <- nrow(X)
d <- ncol(X) + n_rbf

# number of jet/Higgs mass groups to build different models for
G <- 3
thresholds <- c(0.7, 0.4, 0.8)

# choose model
# logreg
# model_init <- partial(logistic_model$new, lambda=lambda)
rm(logistic)
model_init <- partial(logistic_l1_model$new, c=c)

#
result_label <- "_l1_logreg_"

sum_w <- sum(w)
```


Use missing data pattern to partition data
```{r jet/missing}
# get missing rows. separate by number of jets and presence of Higgs mass and fit separate models
# find columns with features with any missing values for each number of jets: 0, 1, 2+ in combination with the presence (or absence) of the Higgs mass, defined by j=1,2,3, 4, 5, 6. i.e. j=1 => nj=0 & mH != -999, j=2 => 
jet_cats <- c(1:3, 1:3)
features_to_rm <- set_features_to_rm(X, G, kI, nj)
```

```{r test, include=FALSE}
midx <- matrix(, nrow=G*K,ncol=2)
  for (j in 1:G) {
    for (k in 1:K) {
      idx <- get_model_idx(j, k, K)
      midx[idx,] <- inv_model_idx(idx, K)
    }
  }
midx
```

Count the number of rows of data in each category and fold, with the first column being the training set and the second being the test for each fold/category
```{r number of rows}
n_rows_p_partition <- matrix(, nrow=G*K, ncol=2)
for (mj in 1:G) {
    # loop over sets of jet number {0, 1, 2+} and mH presence/absence
    for (k in 1:K) {
        j <- jet_cats[mj]
        fit_row_idx <- kI != k & idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)
        test_row_idx <- kI == k & idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)

        model_idx <- get_model_idx(mj, k, K)

        n_rows_p_partition[model_idx, 1] <- sum(fit_row_idx)
        n_rows_p_partition[model_idx, 2] <- sum(test_row_idx)
    }
}
n_rows_p_partition
```

Run models
```{r fitting}
# loop over folds
#create lists to hold the k models and k roc curves
models <- vector("list", G*K)
rocs <- vector("list", G*K)
b <- matrix(, nrow=d, ncol=G*K)

ams <- rep(NA, length=G*K)
auc <- rep(NA, length=G*K)

par(mfrow=c(2, 3))
for (mj in 1:G) {
    # loop over sets of jet number {0, 1, 2+} and mH presence/absence
    for (k in 1:K) {
        j <- jet_cats[mj]
        fit_row_idx <- kI != k & idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)
        test_row_idx <- kI == k & idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)

        # add r RBF centroid features, using the same reference centroids in training and testing sets
        if (n_rbf > 0) {
          rbf_centroids <- get_rbf_centroids(X[fit_row_idx, ], n_rbf)
          Xi <- rbf_centroids$"xi"
          Xtrain <- add_rbf_features(X[fit_row_idx, ], s, n_rbf, Xi=Xi)
          Xtest <- add_rbf_features(X[test_row_idx, ], s, n_rbf, Xi=Xi)
        } else {
          Xtrain <- X[fit_row_idx, ]
          Xtest <- X[test_row_idx, ]
        }
        
        col_idx <- get_valid_cols(colnames(X), features_to_rm, mj)

        Xtrain <- as.matrix(Xtrain[, col_idx])
        Xtest <- as.matrix(Xtest[, col_idx])
        
        Xtest <- scale_dat(Xtest, Xtrain)
        Xtrain <- scale_dat(Xtrain, Xtrain)

        model_idx <- get_model_idx(mj, k, K)

        #fit a logistic regression model to the CV training data
        models[[model_idx]] <- model_init(X=Xtrain, y=y[fit_row_idx])
  
        #use it to predict the classifications of the test data
        p_hat <- models[[model_idx]]$predict(Xtest)

        #create an ROC curve object
        rocs[[model_idx]] <- ROC_curve$new(y[test_row_idx], p_hat)
        
        # probably want to tune the threshold? why does (very) low threshold give better ams?
        y_hat <- decide(p_hat, thresh = thresholds[j])

        ams[model_idx] <- calculate_ams_partition(y[test_row_idx], y_hat, w[test_row_idx], sum_w=sum_w)
    }
}
```
```{r avg auc and plot roc, error=TRUE}
auc <- sapply(rocs, function(x) x$calc_auc())
for(j in 1:G){
  #find average auc over folds
  i1 <- get_model_idx(j, 1, K)
  i2 <- get_model_idx(j, K, K)
  average_auc <- mean(sapply(rocs[i1:i2], function(x) x$auc), na.rm=TRUE)
  #get a plot of the k ROC curves on the same axis
  rocs[[i1]]$plot_curve(title=sprintf("All training data, %i-fold CV, Average AUC %.3f", K, round(average_auc, 3)))
  for (k in 2:K) {
    rocs[[get_model_idx(j, k, K)]]$plot_curve(add=T)
  }
}

```

Fit model (for each category) on whole (unfolded) dataset
```{r full model}
# loop over folds
#create lists to hold the k models and k roc curves
full_models <- vector("list", G)
full_rocs <- vector("list", G)
full_ams_IS <- rep(NA, length=G)
rbf_idx <- matrix(, nrow=G, ncol=n_rbf)
# check warnings?
for (mj in 1:G) {
    # loop over sets of jet number {0, 1, 2+} and mH presence/absence
    j <- jet_cats[mj]
    fit_row_idx <- idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)
        
    if (n_rbf > 0) {
      # add r RBF centroid features, using the same reference centroids in training and testing sets
      rbf_centroids <- get_rbf_centroids(X[fit_row_idx, ], n_rbf)
      Xi <- rbf_centroids$"xi"
      # record which rows to use for OOS in public set (or any other OOS)
      rbf_idx[mj, ] <- rbf_centroids$"idx"
      Xtrain <- add_rbf_features(X[fit_row_idx, ], s, n_rbf, Xi=Xi)
    } else {
          Xtrain <- X[fit_row_idx, ]
    }

    col_idx <- get_valid_cols(colnames(Xtrain), features_to_rm, mj)

    Xtrain <- as.matrix(Xtrain[, col_idx])
    Xtrain <- scale_dat(Xtrain, Xtrain)

    model_idx <- get_model_idx(mj, 1, 1)

    #fit a logistic regression model to the CV training data
    full_models[[model_idx]] <- model_init(X=Xtrain, y=y[fit_row_idx])
}
```

Test on (until now unseen) validation set. Calling it validation as we can then use the summary metrics to tune free parameters.
```{r public}
amsv <- rep(NA, length=G)

# check warnings
sum_wv <- sum(wv)
for (mj in 1:G) {
    j <- jet_cats[mj]
    test_row_idx <- idx_jet_cat(njv, j) & idx_higgs_mass(Xv, mj, G)

    if (n_rbf > 0) {
      # add r RBF centroid features, using the same reference centroids in training and testing sets
      Xi <- X[rbf_idx[mj, ], ]
      Xtest <- add_rbf_features(Xv[test_row_idx, ], s, n_rbf, Xi=Xi)
    } else {
          Xtest <- Xv[test_row_idx, ]
    }
    
    col_idx <- get_valid_cols(colnames(Xtest), features_to_rm, mj)

    Xtest <- as.matrix(Xtest[, col_idx])

    model_idx <- get_model_idx(mj, 1, 1)

    #use it to predict the classifications of the test data
    p_hat <- full_models[[model_idx]]$predict(Xtest)

    #create an ROC curve object
    full_rocs[[model_idx]] <- ROC_curve$new(yv[test_row_idx], p_hat)
    # probably want to tune the threshold? why does (very) low threshold give better ams?
    y_hat <- decide(p_hat, thresh = thresholds[j])

    amsv[model_idx] <- calculate_ams_partition(yv[test_row_idx], y_hat, wv[test_row_idx], sum_w=sum_wv)
}
```

```{r plot public}
for(j in 1:G){
  #find average auc over folds
  full_rocs[[j]]$calc_auc()
  #get a plot of the k ROC curves on the same axis
  full_rocs[[get_model_idx(j, 1, 1)]]$plot_curve(title=sprintf("All training data, public test-set, OOS AUC %.3f, group %i", round(full_rocs[[j]]$auc, 3), j))
}
aucv <- sapply(full_rocs, function(x) x$auc)
```
Summary results:
```{r print}
sprintf("Results for lambda=%.2g, G=%i, n_rbf=%i, K=%i on training set ('%s') and validation set ('%s')", lambda, G, n_rbf, K, train_label, val_label)
sprintf("CV OOS AUC = %.2f ± %.1f", mean(auc), mad(auc))
sprintf("CV OOS AMS = %.2f ± %.1f", mean(ams), mad(ams))
sprintf("Validation set AUC = %.2f ± %.1f", mean(aucv), mad(aucv))
sprintf("Validation set AMS = %.2f ± %.1f", mean(amsv), mad(amsv))
```

```{r record to file}
result <- list("Train"=train_label, "Validation"=val_label, "K"=K, "G"=G, "n_rbf"=n_rbf, "lambda"=lambda, "auc"=mean(auc), "mad(auc)"=mad(auc), "ams"=mean(ams), "mad(ams)"=mad(ams), "aucv"=mean(aucv), "mad(aucv)"=mad(aucv), "amsv"=mean(amsv), "mad(amsv)"=mad(amsv))
write.table(as.data.frame(result), file = sprintf("results%s.csv", result_label), append = TRUE, sep = ",",
eol = "\n", na = "NA", dec = ".", row.names = FALSE,
col.names = FALSE, qmethod = c("escape", "double"))
```

