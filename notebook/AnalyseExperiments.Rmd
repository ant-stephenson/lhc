---
title: "Analyse Experiments"
author: "Anthony Stephenson"
date: "1/18/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r import}
library(ggplot2)
library(dplyr)
library(fs)
library(lhc)
```

# Analyse Experiments

```{r import}
filepath <- path_join(c(dirname(getwd()), "/output/results_experiments4.csv"))
exp_data <- read.csv(filepath)
```

Remove some columns that are not important for this analysis and average over any duplicated experiments:
```{r extract summarise}
exp_data <- exp_data[, !colnames(exp_data) %in% c("c", "Train", "Validation", "model_type")]
exp_data %>% group_by(n_rbf, lambda, poly) %>% summarise_all(funs(mean))
```

Implement a convenient plotting function to summarise results.
```{r plot}
# define function to plot against our metric
plot_metric <- function(data, xlabel, ylabel, loop_label=NULL, filt0=TRUE, ...) {
  if (!is.null(loop_label)) {
    init_idx <- min(data[, loop_label])
    ntrials <- max(data[, loop_label])-(-1 + init_idx)
    filt_func <- function(x) data[, loop_label] == x & filt0
  } else{
    ntrials <- 1
    init_idx <- 0
    filt_func <- function(x) filt0
  }
  
  colours <- generate_colours(ntrials)
  
  ylim <- c(min(data[filt0, ylabel]), max(data[filt0, ylabel]))
  
  filt <- filt_func(init_idx)
  plot(data[filt, xlabel], data[filt, ylabel], col=colours[1], type="lin", xlab=xlabel, ylab=ylabel, main=sprintf("%s vs %s for different %s", ylabel, xlabel, loop_label), ylim=ylim, ...)
  for (i in 1:ntrials) {
    nr <- i + init_idx
    filt <- filt_func(nr)
    lines(data[filt, xlabel], data[filt, ylabel], col=colours[nr])
  }
  legend("bottomleft", legend=c(init_idx:(ntrials+(init_idx-1))), fill=colours)
}
```

Run plots for various permutations of our model parameters to see their relative performance on our key metrics:
```{r run the plots}
# order by our x variable
exp_data <- exp_data[order(exp_data[, "lambda"]), ]

filt0 <- exp_data[, "poly"]==1
plot_metric(exp_data, "lambda", "auc", "n_rbf", filt0, log="x")
plot_metric(exp_data, "lambda", "ams", "n_rbf", filt0, log="x")

# weird oscillations must be due to a change I made to the poly function that removes cols with values > 1e6 (now changed to 1e7). Didn't expect it to make such a difference. Non-deterministic between random subsets of data. A very coarse solutioon but I didn't expect it to matter. Might need to improve
filt0 <- exp_data[, "poly"]==2
plot_metric(exp_data, "lambda", "auc", "n_rbf", filt0, log="x")
plot_metric(exp_data, "lambda", "ams", "n_rbf", filt0, log="x")

filt0 <- exp_data[, "poly"]==3
plot_metric(exp_data, "lambda", "auc", "n_rbf", filt0, log="x")
plot_metric(exp_data, "lambda", "ams", "n_rbf", filt0, log="x")

filt0 <- exp_data[, "n_rbf"]==0
plot_metric(exp_data, "lambda", "auc", "poly", filt0, log="x")
plot_metric(exp_data, "lambda", "ams", "poly", filt0, log="x")

exp_data <- exp_data[order(exp_data[, "poly"]), ]
filt0 <- exp_data[, "n_rbf"]==0
plot_metric(exp_data, "poly", "auc", filt0=filt0)
```

Print results as a LaTeX table that can be copied:
```{r latex output}
library(Hmisc)
# consider scaling metrics by MAD(metric) to get a handle on variation
scaled_auc <- exp_data[,"auc"]/exp_data[,"mad.auc."]
scaled_ams <- exp_data[,"ams"]/exp_data[,"mad.ams."]
exp_data[, "scaled.auc"] <- scaled_auc
exp_data[, "scaled.ams"] <- scaled_ams
#scaled <- head(arrange(exp_data, scaled_ams, scaled_auc))

# sort/filter data to only retain what we want to record and round numerical values to 3d.p for readibility
output <- exp_data[, c("n_rbf", "lambda", "poly", "auc", "mad.auc.", "ams", "mad.ams.", "scaled.auc", "scaled.ams")] %>%
  arrange(desc(scaled.ams), desc(scaled.auc)) %>%
  mutate_if(is.numeric, round, digits=3)

# keep top 5 rows
output <- output[1:5, ]
print(output)

# Generate LaTeX table
latex(output, file="")
```
