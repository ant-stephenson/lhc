---
title: "Feature selection"
author: "Anthony Stephenson"
date: "1/3/2021"
output: pdf_document
---

## Load packages
```{r, echo=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(Matrix)
library(fs)
library(purrr)
library(RColorBrewer)
library(lhc)
```
## Introduction
We decided that running a logistic regression model would be a good, robust model to use as, at the very least, a benchmark. In an effort to include higher order features than the original dataset, we decided to implement feature augmentation in the form of polynomial transformations and RBF centroid features that we could apply easily with the use of a few parameters in the script. The hope was that we could run a series of experiments exploring the parameter space, with 10-fold cross-validation, and analyse the results to obtain a "best" model to apply to a hold-out test set.

The Kaggle data is divided into various subsections, each labelled with a character:
Kaggle sets:  ”t”:training, ”b”:public leaderboard, ”v”:private leaderboard, ”u”:unused.

"t" is used for the cross-validation procedure and as a final full fitting set before testing on the "v" hold-out set. The "b" set is relatively small, so has been excluded for simplicity.

From the exploratory data analysis we saw that the data can be viewed as having been generated from different underlying processes, based on the structure of the missing data. Although we considered building independent models for each of these groups, we found that some of the groups were too small to build reliable models. As a result we opted to divide only by jet number (rather than Higgs mass as well) and remove the remaining missing columns as required. This appeared to provide more robust and better performance.

## Initialise script

Set script parameters:
```{r initialise script params}
# Regularisation (L2) parameter [global]
lambda <- 1e-4
# constraint parameter for L1 Logistic regression [global]
C <- 1
# number of jet/Higgs mass groups to build different models for
G <- 3
poly_order <- 2
n_rbf <- 3

# AMS thresholds
thresholds <- c(0.6, 0.4, 0.6)

# choose model. Interchangeable so long as we have implemented polymorphism across the model classes so that they share a common interface. Use "partial" to call with optional parameters to enforce an interface of f(X,y)
# L2 logistic regression
model_init <- partial(logistic_model$new, lambda=lambda)

# Constrained logistic regression (using CVXR)
# model_init <- partial(logistic_l1_model$new, C=C)

# pick training and validation sets
train_label = c("t")
val_label <- c("v")

# name to use in the .csv output of results
result_label <- "test_private_leaderboard"

# bool to choose whether to save outputs
do_save_outputs <- TRUE

# dir to save figures
fig_dir <- path_join(c(dirname(getwd()), "doc/figs/"))
```

## Load data
Load the data
```{r load data}
# divide data into training kaggle set, and retain hold-out (before further cross-validation partitioning)
source_path <- path_join(c(path_dir(path_dir(getwd()))))
filename <- "atlas-higgs-challenge-2014-v2.csv"
filepath <- path_join(c(source_path, "LHC_dump", "R", filename))
data <- import_data(filepath)
```

Assign variables corresponding to the training set.
```{r training set}
train_idx <- get_subset_idx(data$kaggle_s, train_label)
X <- data$X[train_idx, ]
y <- data$y[train_idx]
# need to use kaggle weignts to make AMS correct?!
kaggle_w <- data$kaggle_w[train_idx]
w <- data$kaggle_w[train_idx]
nj <- data$nj[train_idx]
e_id <- data$e_id[train_idx]
```

Assign variables corresponding to the validation set.
```{r validation set}
# public leaderboard set
val_idx <- get_subset_idx(data$kaggle_s, val_label)
Xv <- data$X[val_idx, ]
yv<- data$y[val_idx]
wv <- data$kaggle_w[val_idx]
njv <- data$nj[val_idx]
```


## Feature engineering
Apply some feature engineering
```{r add features}
# modify features
X <- reduce_features(X)
X <- invert_angle_sign(X)
Xv <- reduce_features(Xv)
Xv <- invert_angle_sign(Xv)

if (poly_order > 1) {
  X <- poly_transform(X, poly_order)
  Xv <- poly_transform(Xv, poly_order)
}

# ensure X and Xv have the same columns
cols2keep <- intersect(colnames(X), colnames(Xv))
Xv <- Xv[, cols2keep]
X <- X[, cols2keep]

Xv <- scale_dat(Xv, X, na.rm=TRUE)
```

Should we include interaction terms? Plot computation scaling as a function of number of initial features $d$ and order of polynomial terms of those to include, if we were to include all pairwise interactions.
```{r scaling of interactions}
nint <- function(d) {
  exp(lgamma(d+1) - log(2) - lgamma(d-1))
}
tscale <- function(d,b) {
  dI <- nint(d)
  1 + dI^3/(b*d)^3 + dI^2/(b*d)^2 + dI/(b*d)
}
d <- 1:30
b <- 1:3
plot(d, tscale(d,b[1]), type="lin", col="blue", log="y")
lines(d, tscale(d,b[2]), col="red")
lines(d, tscale(d,b[3]), col="green")
legend("bottomleft", legend=c("b=1", "b=2", "b=3"), fill=c("blue", "red", "green"))
```

 ## 
Compute some initial parameters
```{r init params}
s <- avg_median_pairwise_distance(X)

# K-Fold CV partitioning
K <- 10
kI <- partition_data(length(y), K, random = TRUE)

# number of rows and columns of data matrix
n <- nrow(X)
d <- ncol(X) + n_rbf

# sum weights for to renormalise for AMS in partitions
sum_w <- sum(w)

# set colours for jet groups
colours <- generate_colours(G)
```

<!-- Take a look at how signal/background events are distributed in terms of jet groups and similarly the weights associated with them. -->
<!-- **THIS SHOULD GO IN EXPLORATORY DATA ANALYSIS** -->
```{r check weight distributions, eval=FALSE}
w_ratio <- function(y, w, nj, label, j) {
  sum(w[idx_jet_cat(nj, j) & y==label])/sum(w[idx_jet_cat(nj, j)])
}
weight_stats <- data.frame(j=c(1,2,3),
                           s=c(w_ratio(y,w,nj,1,1), w_ratio(y,w,nj,1,2), w_ratio(y,w,nj,1,3)), 
                            b=c(w_ratio(y,w,nj,0,1), w_ratio(y,w,nj,0,2), w_ratio(y,w,nj,0,3)) 
)
weight_stats
n_ratio <- function(y, nj, label, j) {
  sum(y==label & idx_jet_cat(nj, j))/sum(idx_jet_cat(nj, j))
}
n_stats <- data.frame(j=c(1,2,3),
                      s=c(n_ratio(y,nj,1,1), n_ratio(y,nj,1,2), n_ratio(y,nj,1,3)), 
                      b=c(n_ratio(y,nj,0,1), n_ratio(y,nj,0,2), n_ratio(y,nj,0,3)) 
)
n_stats
```

Use missing data pattern to partition data.
```{r jet/missing}
# get missing rows. separate by number of jets and presence of Higgs mass and fit separate models
# find columns with features with any missing values for each number of jets: 0, 1, 2+ in combination with the presence (or absence) of the Higgs mass, defined by j=1,2,3, 4, 5, 6. i.e. j=1 => nj=0 & mH != -999, j=2 => 
jet_cats <- c(1:3, 1:3)
features_to_rm <- set_features_to_rm(X, G, kI, nj)
```

<!-- Count the number of rows of data in each category and fold, with the first column being the training set and the second being the test for each fold/category -->

<!-- **THIS SHOULD GO IN EXPLORATORY DATA ANALYSIS** -->
```{r number of rows, eval=FALSE}
n_rows_p_partition <- matrix(, nrow=G*K, ncol=2)
for (mj in 1:G) {
    # loop over sets of jet number {0, 1, 2+} and mH presence/absence
    for (k in 1:K) {
        j <- jet_cats[mj]
        fit_row_idx <- kI != k & idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)
        test_row_idx <- kI == k & idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)

        model_idx <- get_model_idx(mj, k, K)

        n_rows_p_partition[model_idx, 1] <- sum(fit_row_idx)
        n_rows_p_partition[model_idx, 2] <- sum(test_row_idx)
    }
}
n_rows_p_partition
```
## Run CV experiments
Loop over jet groups and folds and fit a model for each (so $GK$ total) and then test each of these on their OOS subset, recording AUC and AMS.
```{r fitting}
# loop over folds
#create lists to hold the k models and k roc curves
models <- vector("list", G*K)
rocs <- vector("list", G*K)
ams_obj <- vector("list", G*K)
b <- matrix(, nrow=d, ncol=G*K)

ams <- rep(NA, length=G*K)
auc <- rep(NA, length=G*K)

par(mfrow=c(2, 3))
for (mj in 1:G) {
    # loop over sets of jet number {0, 1, 2+} and mH presence/absence
    for (k in 1:K) {
        j <- jet_cats[mj]
        # get train and test split indices for this jet category (and/or higgs mass) and fold
        fit_row_idx <- kI != k & idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)
        test_row_idx <- kI == k & idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)

        # add r RBF centroid features, using the same reference centroids in training and testing sets
        if (n_rbf > 0) {
          rbf_centroids <- get_rbf_centroids(X[fit_row_idx, ], n_rbf)
          Xi <- rbf_centroids$"xi"
          Xtrain <- add_rbf_features(X[fit_row_idx, ], s, n_rbf, Xi=Xi)
          Xtest <- add_rbf_features(X[test_row_idx, ], s, n_rbf, Xi=Xi)
        } else {
          Xtrain <- X[fit_row_idx, ]
          Xtest <- X[test_row_idx, ]
        }
        
        # get indices for columns to include for this jet category
        col_idx <- get_valid_cols(colnames(X), features_to_rm, mj)

        # define train and test matricies
        Xtrain <- as.matrix(Xtrain[, col_idx])
        Xtest <- as.matrix(Xtest[, col_idx])
        
        # standardize the data (using training as a reference for the test set to avoid using any test information)
        Xtest <- scale_dat(Xtest, Xtrain)
        Xtrain <- scale_dat(Xtrain, Xtrain)

        # get index of this model, from jet category and fold (to retrieve in results arrays)
        model_idx <- get_model_idx(mj, k, K)

        # fit a logistic regression model to the CV training data
        models[[model_idx]] <- model_init(X=Xtrain, y=y[fit_row_idx])
  
        # use it to predict the classifications of the test data
        p_hat <- models[[model_idx]]$predict(Xtest)

        # create an ROC curve object
        rocs[[model_idx]] <- ROC_curve$new(y[test_row_idx], p_hat)

        # create an AMS curve object
        w_this_partition <- w[test_row_idx] * sum_w/sum(w[test_row_idx])
        ams_obj[[model_idx]] <- AMS_data$new(y[test_row_idx], p_hat, w_this_partition)#, sum_w=sum_w)
    }
}
```
## Plot CV metrics
Plot ROC curves for each fold for each group and calculate mean AUC over folds for each group.
```{r avg auc and plot roc, error=TRUE}
auc <- sapply(rocs, function(x) x$calc_auc())
for(j in 1:G){
  #find average auc over folds
  i1 <- get_model_idx(j, 1, K)
  i2 <- get_model_idx(j, K, K)
  average_auc <- mean(sapply(rocs[i1:i2], function(x) x$auc), na.rm=TRUE)
  #get a plot of the k ROC curves on the same axis
  if (do_save_outputs) {
    save_fig(partial(plot_rocs, rocs[i1:i2], title=sprintf("All training data, %i-fold CV, Average AUC %.3f", K, round(average_auc, 3))), 
             path_join(c(fig_dir, sprintf("cvoos-auc-curves%i.pdf", j))))
  }
  # pdf(file=path_join(c(fig_dir, sprintf("cvoos-auc-curves%i.pdf", j))))
  plot_rocs(rocs[i1:i2], title=sprintf("All training data, %i-fold CV, Average AUC %.3f", K, round(average_auc, 3)))
  # dev.off()
}
```
If we now also plot curves of the AMS against threshold we can get a feeling for what we should choose and how consistent they are across folds (for each model type). They are all relatively noisy, although `j=3` is comfortably the worst of the three in this respect. In an effort to account for this, and reduce the chance of overfitting the thresholds, we have taken to calculating the minimum curve over the K fold curves (by taking the minimum over folds at each threshold point) and then finding the maximum threshold for this curve. The results seem more robust, and give approximately $(0.6, 0.4, 0.6)$.
```{r avg ams and plot roc, error=TRUE}
ams <- sapply(ams_obj, function(x) x$calc_ams())
par(mar=c(3,3,3,3))
for(j in 1:G){
  #find average auc over folds
  i1 <- get_model_idx(j, 1, K)
  i2 <- get_model_idx(j, K, K)
  # calculate the minimum AMS over the curves at each point to get an overall lower bound curve
  #get a plot of the k ROC curves on the same axis
  if (do_save_outputs) {
    save_fig(partial(plot_amss, ams_obj[i1:i2]), 
             path_join(c(fig_dir, sprintf("cvoos-ams-curves%i.pdf", j))))
  }
  # pdf(file=path_join(c(fig_dir, sprintf("cvoos-ams-curves%i.pdf", j))))
  plot_amss(ams_obj[i1:i2])
  # dev.off()
}
```

## Run full model
Fit model (for each category) on whole (unfolded) dataset
```{r full model}
# loop over folds
#create lists to hold the k models and k roc curves
full_models <- vector("list", G)
full_rocs <- vector("list", G)
rbf_idx <- matrix(, nrow=G, ncol=n_rbf)
# check warnings?
for (mj in 1:G) {
    # loop over sets of jet number {0, 1, 2+} and mH presence/absence
    j <- jet_cats[mj]
    fit_row_idx <- idx_jet_cat(nj, j) & idx_higgs_mass(X, mj, G)
        
    if (n_rbf > 0) {
      # add r RBF centroid features, using the same reference centroids in training and testing sets
      rbf_centroids <- get_rbf_centroids(X[fit_row_idx, ], n_rbf)
      Xi <- rbf_centroids$"xi"
      # record which rows to use for OOS in public set (or any other OOS)
      rbf_idx[mj, ] <- rbf_centroids$"idx"
      Xtrain <- add_rbf_features(X[fit_row_idx, ], s, n_rbf, Xi=Xi)
    } else {
          Xtrain <- X[fit_row_idx, ]
    }

    # get indices for columns to include for this jet category
    col_idx <- get_valid_cols(colnames(Xtrain), features_to_rm, mj)

    # standardize the data 
    Xtrain <- as.matrix(Xtrain[, col_idx])
    Xtrain <- scale_dat(Xtrain, Xtrain)

    # get index of this model, from jet category and fold (to retrieve in results arrays)
    model_idx <- get_model_idx(mj, 1, 1)

    #fit a logistic regression model to the all of the training data
    full_models[[model_idx]] <- model_init(X=Xtrain, y=y[fit_row_idx])
}
```

Test on (until now unseen) validation set. Calling it validation as we can then use the summary metrics to check free parameters, being careful not to overdo it.
```{r validation}
amsv_obj <- vector("list", G)

# check warnings
sum_wv <- sum(wv)
for (mj in 1:G) {
    j <- jet_cats[mj]
    test_row_idx <- idx_jet_cat(njv, j) & idx_higgs_mass(Xv, mj, G)

    if (n_rbf > 0) {
      # add r RBF centroid features, using the same reference centroids in training and testing sets
      Xi <- X[rbf_idx[mj, ], ]
      Xtest <- add_rbf_features(Xv[test_row_idx, ], s, n_rbf, Xi=Xi)
    } else {
          Xtest <- Xv[test_row_idx, ]
    }
    
    col_idx <- get_valid_cols(colnames(Xtest), features_to_rm, mj)

    Xtest <- as.matrix(Xtest[, col_idx])

    model_idx <- get_model_idx(mj, 1, 1)

    #use it to predict the classifications of the test data
    p_hat <- full_models[[model_idx]]$predict(Xtest)

    #create an ROC curve object
    full_rocs[[model_idx]] <- ROC_curve$new(yv[test_row_idx], p_hat)

    w_this_partition <- wv[test_row_idx] * sum_wv/sum(wv[test_row_idx])
    amsv_obj[[model_idx]] <- AMS_data$new(yv[test_row_idx], p_hat, w_this_partition)#, sum_w=sum_wv)
}
```

## Plot test metrics
Plot ROC curves for each group on the validation set, with AUC for each.
```{r plot validation auc}
. <- sapply(full_rocs, function(x) x$calc_auc())
aucv <- sapply(full_rocs, function(x) round(x$auc, 3))
# pdf(file=path_join(c(fig_dir, "validation-auc-curves.pdf")))
par(mar=c(4,4,4,4))
if (do_save_outputs) {
  save_fig(partial(plot_rocs, full_rocs, title=sprintf("All training data, %i-fold CV, AUC (%.3f, %.3f, %.3f)", K, round(aucv[1], 3), round(aucv[2], 3), round(aucv[3], 3))), 
           path_join(c(fig_dir, "validation-auc-curves.pdf")))
}
plot_rocs(full_rocs, title=sprintf("All training data, %i-fold CV, AUC (%.3f, %.3f, %.3f)", K, round(aucv[1], 3), round(aucv[2], 3), round(aucv[3], 3)))
# dev.off()
```
Now plotting the AMS thresholds for the validation set, we see that they differ substantially from those we saw in the training set, despite the K-fold CV. The validation set is much smaller than the full training set however, so we would expect it to be less reliable due to randomness in the data set. 
```{r plot validation ams thresholds}
amsv <- sapply(amsv_obj, function(x) x$calc_ams())
if (do_save_outputs) {
  save_fig(partial(plot_amss, amsv_obj, min.max=FALSE), 
           path_join(c(fig_dir, "validation-ams-curves.pdf")))
}
# pdf(file=path_join(c(fig_dir, "validation-ams-curves.pdf")))
plot_amss(amsv_obj, min.max=FALSE)
# dev.off()
```
## Summary results
Summary results:
```{r print}
get_threshold_ams <- function(ams_mat, G, K, thresholds) {
  ams <- rep(NA, G * K)
  for (g in 1:G) {
    thresh_idx <- which.min(abs(ams_obj[[1]]$thresholds - thresholds[g]))
    model_set <- ((g-1)*K+1):(g*K)
    ams[model_set] <- ams_mat[thresh_idx, model_set]
  }
  return(ams)
}

get_mean_mad_ams <- function(ams, G, K) {
  mads <- rep(NA, G)
  for (g in 1:G) {
    model_set <- ((g-1)*K+1):(g*K)
    mads[g] <- mad(ams[model_set])
  }
  return(mean(mads))
}

result_ams <- get_threshold_ams(ams, G, K, thresholds)
mad_ams <- get_mean_mad_ams(result_ams, G, K)

result_amsv <- get_threshold_ams(amsv, G, 1, thresholds)

sprintf("Results for lambda=%.2g, G=%i, n_rbf=%i, K=%i on training set ('%s') and validation set ('%s')", lambda, G, n_rbf, K, train_label, val_label)
sprintf("CV OOS AUC = %.2f ± %.1f", mean(auc), mad(auc))
sprintf("CV OOS AMS = %.2f ± %.1f", mean(result_ams), mad_ams)
sprintf("Validation set AUC = %.2f", mean(aucv))
sprintf("Validation set AMS = %.2f", mean(result_amsv))
```
Save final results to a LaTeX table.
```{r latex out}
library(Hmisc)

# keep top 5 rows
output <- t(matrix(c(mean(auc), mad(auc), mean(result_ams), mad_ams, mean(aucv), NA, mean(result_amsv), NA), nrow=4))
colnames(output) <- c("AUC", "mad.AUC", "AMS", "mad.AMS")
output <- data.frame(output)
output <- mutate_if(output, is.numeric, round, digits=3)
row.names(output) <- c("CV OOS", "Test set")

# Generate LaTeX table
if (do_save_outputs) {
  latex(output, file=path_join(c(dirname(getwd()), "doc/final_results_table.tex")), caption=sprintf("Results for $\\lambda=%.2g$, $G=%i$, $n_{rbf}=%i$, K=%i on training set (`%s') and validation set (`%s')", lambda, G, n_rbf, K, train_label, val_label), label="table:final_results")
}
```

Save the output results to a .csv file.
```{r record to file}
result <- list("Train"=train_label, "Validation"=val_label, "K"=K, "G"=G, "n_rbf"=n_rbf, "lambda"=lambda, "auc"=mean(auc), "mad(auc)"=mad(auc), "ams"=mean(ams), "mad(ams)"=mad(ams), "aucv"=mean(aucv), "mad(aucv)"=mad(aucv), "amsv"=mean(amsv), "mad(amsv)"=mad(amsv))
write.table(as.data.frame(result), file = sprintf("/output/results_%s.csv", result_label), append = TRUE, sep = ",",
eol = "\n", na = "NA", dec = ".", row.names = FALSE,
col.names = FALSE, qmethod = c("escape", "double"))
```

